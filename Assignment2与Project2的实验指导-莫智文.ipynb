{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验指导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验目的：\n",
    "\n",
    "1. 掌握神经网络的基本原理；\n",
    "2. 掌握word2vec的基本原理；\n",
    "3. 掌握Keras的基本用法；\n",
    "4. 掌握tensorflow的基本用法；\n",
    "5. 掌握RNN的基本原理；\n",
    "6. 观察熟悉RNN的两种常见变体(LSTM和GRU)的原理和用法；\n",
    "7. 熟悉深度学习自然语言处理的基本流程。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I 实验描述：\n",
    "\n",
    "#### 数据： 使用爬虫获得的豆瓣评论数据\n",
    "#### 目标： 建立机器学习模型，能够对输入的句子自动判断其对应的分值或感情倾向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II 数据预处理\n",
    "\n",
    "目的： 将文本信息变成神经网络可以处理的数据格式； "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分： 基础理论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III 神经网络的基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: 神经网络的Loss函数的作用为何？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答： \n",
    "\n",
    "衡量神经网络预测值和真实值之间的误差，作为优化的目标函数，计算出来的误差值是反向传播的依据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: 神经网络的激活函数(activation function)起什么作用？ 如果没有激活函数会怎么样？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "1. 激活函数对输入数据进行了非线性变换。\n",
    "\n",
    "2. 若没有激活函数会导致模型过于简单，无法拟合非线性数据的复杂情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: 神经网络的softmax如何理解， 其作用是什么？ 在`答案`中写出softmax的python表达；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答案：\n",
    "\n",
    "softmax函数把神经网络输出的logits转化为总和为1的概率分布。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "logits = np.array([y1,y2,y3])\n",
    "softmax = np.exp(logits)/np.sum(np.exp(logits))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: 简述 normalized_1 和softmax函数的相同点和不同点， 说明softmax相比normalized_1该函数的优势所在\n",
    "```python\n",
    "output = np.array([y1, y2, y3])\n",
    "\n",
    "normalized_1 = output / np.sum(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "相同点：normalized_1和softmax函数都是数据归一化的方式。\n",
    "\n",
    "不同点：softmax在标准化之前对数据进行了指数转换。\n",
    "\n",
    "softmax的优势：可以处理output中同时存在正负数的情况，数据间评分的概率分布不会在计算时抵消。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5: 写出crossentropy的函数表达式，说明该函数的作用和意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "cross_entropy $:= -\\sum_{c=1}^{C}y_{c}\\log{\\hat{y}_{c}}$\n",
    "\n",
    "在分类问题中对比预测值和真实值衡量它们之间的误差。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV 掌握word2vec的基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6: 说明word2vec要解决的问题背景， 以及word2vec的基本思路， 说明word2vec比起之前方法的优势；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "问题背景：\n",
    "1. 对于简单的方法，词向量化质量低，没有很好地保持不同词语之间的语义关系。\n",
    "2. 过于复杂的方法，当需要向量化的词汇太多时计算量过大，效率低。\n",
    "\n",
    "基本思路：\n",
    "1. 对比不同模型**NNLM**,**RNNLM**的计算复杂度，非线性隐藏层构成了主要的计算复杂度，需要提出一个更有效率的模型。\n",
    "2. Log-linear models:**CBOW**和**Skip-gram**,两个模型的网络结构都相同，都是一层隐藏层作线性投影后再用log-linear输出分类结果,不同的是**CBOW**是用前后内容预测当前词，而**Skip-gram**是用当前词预测围绕再其附近的词。\n",
    "\n",
    "优势：\n",
    "1. 减少了计算复杂度的同时，提高了模型准确率。\n",
    "2. 更进一步地，训练出来的词向量能以简单代数运算的方式表达词义，例如Paris - France + Italy = Rome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7: 说明word2vec的预测目标， predication target, 在答案中写出skip-gram和cbow的预测概率；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答： \n",
    "1.  skip-gram, $\\hat{y}= \\{P(w_{t-i}|w_t),P(w_{t-i+1}|w_t),\\cdots P(w_{t-1}|w_t),P(w_{t+1}|w_t) \\cdots ,P(w_{t+i}|w_t)\\}$\n",
    "2.  cbow, $\\hat{y} = P(w_t|w_{t-i},w_{t-i+1},\\cdots w_{t-1},w_{t-1}\\cdots,w_{t+i})$\n",
    "\n",
    "$i$ is window size\n",
    "\n",
    "hints: 你可能需要查询latex的基本写法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8: 请说明word2vec的两种常见优化方法，分别阐述其原理；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "\n",
    "1. Hierachical softmax. 将原本要预测的词汇集合$V$转化为binary tree的搜索路径，每一个词都对应着一条唯一的路径，预测词出现的概率就相当于预测这条路径出现的概率。这样作当更新参数时，复杂度从原来的$O(V)$减少到了$O(\\log{V})$，大大较少了计算复杂度。\n",
    "\n",
    "2.Negative sampling. 由于需要更新的参数太多，在进行优化时仅仅更新正类和抽样地更新某些负类有关的参数，其中更新的负类可以是随机抽样的，也可以是根据词频调整的函数分布抽样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9: 请说明word2vec中哈夫曼树的作用；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "word2vec使用hierachical softmax进行优化时，需要把预测的词汇转化为binary tree，这个时候通常会采用huffman binary tree。在构造huffman binary tree时，出现频率越高的词汇约接近根节点，而出现频率越低的词汇越接近叶节点，并且保证每个父节点都有两个子节点。在计算结果和优化word2vec时需要遍历从根节点到每个子节点的唯一路径，而只有很少的情况下会达到深层的叶子节点，所以huffman binary tree提高了模型的运算效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10: 在gensim中如何实现词向量？ 请将gensim中实现词向量的代码置于`答案`中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentence =['some sentences']\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11: 请说出除了 skip-gram和cbow的其他4中词向量方法的名字， 并且选取其中两个叙述其基本原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "ont-hot,count vector,co-occurrence vector,tf-idf\n",
    "\n",
    "co-occurence vector:\n",
    "在由两个词在窗口范围内共同出现次数构成的矩阵，通过svd变换后求得的词向量。\n",
    "\n",
    "tf-idf: term frequency-inverse document frequency\n",
    "\n",
    "$TF_i = \\frac{c_{i}}{C_d} $ 词i出现在文档中的频率\n",
    "\n",
    "$IDF_i = \\log({\\frac{N}{c_{i \\in d}}} )$ 总文档/词i出现的文档数\n",
    "\n",
    "$TF-IDF_i = TF_i * IDF_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V 掌握keras的基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12. 参考keras参考手册，构建一个机器学习模型，该模型能够完成使用DNN(deep neural networks) 实现MNIST数据集的分类；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：代码请置于下方：\n",
    "\n",
    "hints:  keras 序列模型构建 https://keras.io/getting-started/sequential-model-guide/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=np.load('/Users/mozhiwen/.keras/datasets/mnist/mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test = f['x_train'],f['y_train'],f['x_test'],f['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat(dataset,labels):\n",
    "    dataset = dataset.reshape(-1,img_size*img_size).astype('float32')\n",
    "    dataset = (dataset-np.min(dataset))/(np.max(dataset)-np.min(dataset))\n",
    "    labels = keras.utils.to_categorical(labels[:,None], num_classes)\n",
    "    return dataset,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train = reformat(x_train,y_train)\n",
    "x_test,y_test = reformat(x_test,y_test)\n",
    "print(x_train.shape[0],'train samples')\n",
    "print(x_test.shape[0],'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512,activation='relu',input_shape=(img_size*img_size,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.2471 - acc: 0.9237 - val_loss: 0.1005 - val_acc: 0.9687\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1046 - acc: 0.9691 - val_loss: 0.0744 - val_acc: 0.9763\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0769 - acc: 0.9770 - val_loss: 0.0753 - val_acc: 0.9768\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.0607 - acc: 0.9817 - val_loss: 0.0773 - val_acc: 0.9802\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0512 - acc: 0.9846 - val_loss: 0.0796 - val_acc: 0.9789\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0433 - acc: 0.9871 - val_loss: 0.0694 - val_acc: 0.9827\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0398 - acc: 0.9889 - val_loss: 0.0777 - val_acc: 0.9825\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0323 - acc: 0.9903 - val_loss: 0.0815 - val_acc: 0.9809\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0323 - acc: 0.9910 - val_loss: 0.0963 - val_acc: 0.9808\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0294 - acc: 0.9910 - val_loss: 0.0860 - val_acc: 0.9819\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0275 - acc: 0.9923 - val_loss: 0.0897 - val_acc: 0.9823\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0250 - acc: 0.9932 - val_loss: 0.1038 - val_acc: 0.9816\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.0246 - acc: 0.9931 - val_loss: 0.1057 - val_acc: 0.9825\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0215 - acc: 0.9943 - val_loss: 0.1016 - val_acc: 0.9821\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.0241 - acc: 0.9935 - val_loss: 0.1068 - val_acc: 0.9810\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0218 - acc: 0.9941 - val_loss: 0.0926 - val_acc: 0.9847\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0196 - acc: 0.9952 - val_loss: 0.1092 - val_acc: 0.9816\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0221 - acc: 0.9940 - val_loss: 0.1044 - val_acc: 0.9836\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0205 - acc: 0.9946 - val_loss: 0.1141 - val_acc: 0.9848\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0213 - acc: 0.9946 - val_loss: 0.0974 - val_acc: 0.9842\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                    verbose = 1,\n",
    "                    validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0973729898917102\n",
      "Test accuracy: 0.9842\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:',score[0])\n",
    "print('Test accuracy:',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI 掌握tensorflow的基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13: 参考tensorflow的参考手册，构建一个机器学习模型，该模型能够完成使用DNN(deep neural networks)实现MNIST数据集的分类；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：代码请置于下方：\n",
    "\n",
    "hints:tensorflow实现MNIST https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #input\n",
    "    train_data = tf.placeholder(tf.float32,shape=(None,img_size*img_size))\n",
    "    train_labels = tf.placeholder(tf.float32,shape=(None,num_classes))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    test_data = tf.constant(x_test)\n",
    "    test_labels = tf.constant(y_test)\n",
    "    \n",
    "    #variables\n",
    "    w1 = tf.Variable(tf.truncated_normal([img_size * img_size, 512]))\n",
    "    b1 = tf.Variable(tf.zeros([512]))\n",
    "    w2 = tf.Variable(tf.truncated_normal([512, 512]))\n",
    "    b2 = tf.Variable(tf.zeros([512]))\n",
    "    w3 = tf.Variable(tf.truncated_normal([512, num_classes]))\n",
    "    b3 = tf.Variable(tf.zeros([num_classes]))\n",
    "    \n",
    "    #model\n",
    "    layer1 = tf.nn.relu(train_data@w1+b1)\n",
    "    drop_layer1 = tf.nn.dropout(layer1,keep_prob=keep_prob)\n",
    "    layer2 = tf.nn.relu(drop_layer1@w2+b2)\n",
    "    drop_layer2 = tf.nn.dropout(layer2,keep_prob=keep_prob)\n",
    "    logits = drop_layer2@w3+b3\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=train_labels,logits=logits))\n",
    "    \n",
    "    optimizer = tf.train.RMSPropOptimizer(0.001,decay=0.0).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    test_out1 = tf.nn.relu(test_data@w1+b1)\n",
    "    test_out2 = tf.nn.relu(test_out1@w2+b2)\n",
    "    test_logits = test_out2@w3+b3\n",
    "    test_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=test_labels,logits=test_logits))\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(dataset,labels,batch_size):\n",
    "    batch_num = dataset.shape[0] // batch_size\n",
    "    sample_num = batch_size * batch_num\n",
    "    for i in range(batch_num):\n",
    "        x = dataset[i*batch_size:(i+1)*batch_size,:]\n",
    "        y = labels[i*batch_size:(i+1)*batch_size,:]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 58.746006 accuracy 90.6%\n",
      "epoch 1 loss: 21.229315 accuracy 93.0%\n",
      "epoch 2 loss: 25.321468 accuracy 92.2%\n",
      "epoch 3 loss: 22.862274 accuracy 93.8%\n",
      "epoch 4 loss: 11.274590 accuracy 94.5%\n",
      "epoch 5 loss: 7.947142 accuracy 96.9%\n",
      "epoch 6 loss: 2.745044 accuracy 97.7%\n",
      "epoch 7 loss: 10.025577 accuracy 96.9%\n",
      "epoch 8 loss: 1.138198 accuracy 97.7%\n",
      "epoch 9 loss: 7.077971 accuracy 97.7%\n",
      "epoch 10 loss: 4.300566 accuracy 96.9%\n",
      "epoch 11 loss: 0.718770 accuracy 98.4%\n",
      "epoch 12 loss: 3.604064 accuracy 97.7%\n",
      "epoch 13 loss: 6.006754 accuracy 96.9%\n",
      "epoch 14 loss: 3.026232 accuracy 96.9%\n",
      "epoch 15 loss: 2.748903 accuracy 98.4%\n",
      "epoch 16 loss: 2.132647 accuracy 98.4%\n",
      "epoch 17 loss: 2.855407 accuracy 96.1%\n",
      "epoch 18 loss: 4.587665 accuracy 96.9%\n",
      "epoch 19 loss: 1.937775 accuracy 96.9%\n",
      "Test loss: 5.906247 accuracy 96.0%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    step = 0\n",
    "    for i in range(epochs):\n",
    "        for x,y in get_batches(x_train,y_train,batch_size):\n",
    "            feed_dict = {train_data:x,train_labels:y,keep_prob:0.8}\n",
    "            _,l,predictions = session.run([optimizer,loss,train_prediction],\n",
    "                                                feed_dict=feed_dict)\n",
    "        print('epoch %d loss: %f accuracy %.1f%%' % (i,l,accuracy(predictions, y)))\n",
    "        \n",
    "    print('Test loss: %f accuracy %.1f%%' % (test_loss.eval(),accuracy(test_prediction.eval(), y_test)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q14: 参考keras和tensorflow对同一问题的实现，说明keras和tensorflow的异同；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：Q12,Q12分别用keras和tensorflow实现了dnn识别mnist数据集。相同点是都需要自己定义模型的输入输出，损失函数和优化器。不同点是相比起tensorflow，keras提供了许多常用的api，实现起来代码量少，代码实现的模型简洁易懂，比较简便省事，不需要对模型和编程有深入的了解也能使用。而tensrflow相比起来更加强大，提供了很多在keras下无法掌控的个性化的参数设置，实现同一个任务代码量比较多，使用要求对模型和框架理解比较深入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q15: Q12， Q13 的tensorflow 或 keras 模型的训练时准确率和测试集准确率分别是多少？\n",
    "回答：\n",
    "tensorflow：训练准确率：96.9% 测试准确率：96.0%\n",
    "keras：训练准确率：97.4% 测试准确率：98.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q16: 训练时准确率大于测试集准确率的现象叫什么名字，在神经网络中如何解决该问题？\n",
    "回答：过拟合。使用early stopping,l2-regularization,drop out。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q17: 请使用自己的语言简述通过正则化 (regularization)减小过拟合的原理；\n",
    "回答：\n",
    "\n",
    "1. early stopping是在训练时观察training set 和 validation set的表现当validation set的表现无明显提升时停止训练。\n",
    "\n",
    "2. l2-regularizarion,改写loss function为 $loss = f_{loss}+\\lambda\\lVert \\omega \\rVert_{2}^{2} $ 其中$f_{loss}$是未加上正则项之前的loss function，$\\lambda\\lVert \\omega \\rVert_{2}^{2}$是正则项，$\\omega$是模型参数。加上了正则项的训练函数，在训练会权衡误差的下降和参数增加对loss fuction的影响，在下降误差的同时尽量使模型参数变得稀疏，避免过拟合。\n",
    "\n",
    "3. 训练阶段，在深度学习模型层与层的输出和输入之间按比例随机切断一些链接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q18: 在tensorflow官方实例中给出的fully connected 神经网络的分类模型中，数据进行了哪些预处理，这些预处理的原因是什么？ \n",
    "回答：对数据进行了标准化，标准化后的数据不受features单位影响，训练时收敛得比较快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII 掌握RNN的基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q19: 简述RNN解决的问题所具有的特点；\n",
    "回答：RNN解决问题的特点是，处理的输入或输出数据都是时间或者序列相关，不同时间位置的数据可能相互关联相互影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q20: 写出RNN实现时间或者序列相关的数学实现(见课程slides)；\n",
    "回答：\n",
    "\n",
    "Elman network:\n",
    "\n",
    "$h_t = \\sigma_{h}({W_hx_t+U_hh_{t-1}+b_h})$\n",
    "\n",
    "$y_t = \\sigma_y(W_yh_t+b_y)$\n",
    "\n",
    "Jordan network:\n",
    "\n",
    "$h_t = \\sigma_h(W_hx_t+U_hy_{t-1}+b_h)$\n",
    "\n",
    "$y_t = \\sigma_y(W_yh_t+b_y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q20: 简述RNN的两种重要变体的提出原因和基本原理？\n",
    "回答：\n",
    "\n",
    "提出原因：\n",
    "\n",
    "在输入长序列训练RNN时，需要展开计算多次使得RNN变成一个很深层的网络，所以非常容易出现梯度消失或梯度爆炸的问题，而且模型的计算和训练会非常缓慢。另一方面，RNN处理长序列时会丧失长期记忆的能力，序列中较前的输入会逐渐被遗忘，这对模型的效果会造成很大的影响。\n",
    "\n",
    "基本原理:\n",
    "\n",
    "LSTM:\n",
    "\n",
    "- 最主要的一层是$g_t$,这层和基础RNN cell基本相同，都是用过$x_t$和$h_{t-1}$得出结果。\n",
    "在此基础上还有三种gate contrillers，用sigmoid函数输出0-1的结果来控制输入输出。\n",
    "- forget gate 控制长期记忆中的哪些部分需要舍弃\n",
    "- input gate 控制 $g_t$中的哪些部分需要加到长期记忆中\n",
    "- output gate 控制更新后的长期记忆哪一部分输出到$h_t$,$y_t$中\n",
    "\n",
    "GRU:\n",
    "\n",
    "- GRU是LSTM的精简版，它把长短期记忆都融合成一个状态$h_t$\n",
    "- 由一个controller同时控制forget gate和input gate\n",
    "- 没有output gate 状态会根据序列时间改变\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q21: Attentional RNN 以及 Stacked RNN 和 Bi-RNN 分别是什么，其做了什么改动？\n",
    "回答：\n",
    "\n",
    "Attentional RNN:  \n",
    "\n",
    "在计算输出值时的在考虑原有输入的同时，加入了context vector $s_{i-1} = f(s_{i-1},y_{i-1},c_i)$,其中$c_i$是源输入的$h_t$的加权平均$c_i = \\sum_{j=1}^{T_x}\\alpha_{ij}h_j$,而$\\alpha_{ij} = softmax(e_{ij})$, $e_{ij} = a(s_{i-1},h_j)$,其中a是一个全链接神经网络。这样做可以建立在计算输出时，建立与源输入的联系，并且专注与这一时刻有用的信息。  \n",
    "\n",
    "Stacked RNN:\n",
    "\n",
    "可以理解为RNN cell的堆叠，t时刻的的输出不仅仅是t+1时刻的输入，同时候也是下一层RNN cell的输入，这样可以根据需要增加网络深度。  \n",
    "\n",
    "Bi-RNN:\n",
    "\n",
    "在原有RNN的基础上，计算从后往前的RNN cell，t时刻的预测值就包含了输入序列的前后信息。t时刻的预测可表示为$\\hat{y_t} = \\sigma_y(w_y[\\overrightarrow{h_t},\\overleftarrow{h_t}]+b_y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分： 实验过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIX 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q22：要实现文本分类或情感分类，文本信息需要进行哪些初始化操作？自己手工实现，keras提供的API，tenorflow提供的API，分别是哪些？请提供关键代码置于下边`回答`中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "1. 简述所需要的初始化操作：  \n",
    "    + 切分词，可根据需要去掉停用词或标点符号\n",
    "    + 构造word-id，id-word的hash表\n",
    "    + 用构造的hash表把句子转化成整数序列\n",
    "    + 准备好预训练的embedding(optional)\n",
    "    + 构造batch函数决定如何输入训练数据  \n",
    "    \n",
    "切分词使用jieba分词\n",
    "\n",
    "keras提供的api：\n",
    "+ keras.preprocessing.text 提供Tokenizer类可以文本构造需要的hash表并把句子转化为整数序列\n",
    "+ keras.preprocessing.sequence 提供pad_sequence函数为序列提供padding\n",
    "+ mini-batch：keras在训练时只需要指定batch size即可，也可以根据自己的需要写一个generator输入训练数据\n",
    "\n",
    "tensorflow提供的api： \n",
    "+ tensorflow 集成了keras的api，对于文本处理大同小异\n",
    "+ 训练时需要自定义batch函数，一般是写一个generator生成需要的训练数据\n",
    "\n",
    "\n",
    "2. 自己手工实现的id_to_word, word_to_id, padding, batched等操作如何实现？\n",
    "    + id_to_word, word_to_id\n",
    "    \n",
    "    ```python\n",
    "        word_count = Counter()\n",
    "    for s in movie_comments['word_sequence']:\n",
    "        for w in s.split():\n",
    "            word_count[w] += 1\n",
    "\n",
    "    id_to_word = {}\n",
    "    id_to_word[0] = '<PAD>'\n",
    "    for i,w in enumerate(word_count,1):id_to_word[i] = w\n",
    "\n",
    "    word_to_id = {w:i for i,w in id_to_word.items()}```\n",
    "    + padding\n",
    "    \n",
    "    ```python\n",
    "    def pad_sequence(word_sequence,maxlength):\n",
    "    \n",
    "        pad_sequence = np.zeros((len(word_sequence),maxlength),dtype=np.int32)\n",
    "        for ri,row in enumerate(word_sequence):\n",
    "            for ci,w in enumerate(row.split()[:maxlength]):\n",
    "                pad_sequence[ri,ci] = word_to_id[w]\n",
    "        return pad_sequence\n",
    "        \n",
    "    sequence = pad_sequence(movie_comments['word_sequence'],200)\n",
    "    ```\n",
    "    \n",
    "    + batched\n",
    "    \n",
    "    ```python\n",
    "    star = movie_comments['star'].values\n",
    "    star = star.astype(np.int32)\n",
    "    sub_one = np.vectorize(lambda x:x-1)\n",
    "    star = sub_one(star)\n",
    "\n",
    "    onehot_star = np.eye(5)[star]\n",
    "\n",
    "    def get_sequence_batches(datasets,labels,batch_size):\n",
    "        assert datasets.shape[0] == labels.shape[0]\n",
    "        batch_num = datasets.shape[0] // batch_size\n",
    "        datasets_cut = datasets[:batch_num*batch_size,:]\n",
    "        last_datasets = datasets[batch_num*batch_size:,:]\n",
    "        labels_cut = labels[:batch_num*batch_size,:]\n",
    "        last_labels = labels[batch_num*batch_size:,:]\n",
    "    \n",
    "    for i in range(batch_num+1):\n",
    "        if i == batch_num:\n",
    "            yield last_datasets,last_labels\n",
    "            break\n",
    "        x = datasets_cut[i*batch_size:(i+1)*batch_size,:]\n",
    "        y = labels_cut[i*batch_size:(i+1)*batch_size,:]\n",
    "        yield x,y\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hints: \n",
    "\n",
    "+ 参考1 https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "+ 参考2 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for sentimental analysis(preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from opencc import OpenCC\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_char(string):\n",
    "    return re.sub('[a-zA-Z0-9]','',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2s(string):\n",
    "    opencc = OpenCC('t2s')\n",
    "    return opencc.convert(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word to id,id to word"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "movie_comments = pd.read_csv('./movie_comments.csv')\n",
    "\n",
    "movie_comments = movie_comments.dropna(subset=['comment','star'])\n",
    "\n",
    "movie_comments = movie_comments.drop(568,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def cut(string): return ' '.join([w for w in jieba.cut(string)])\n",
    "\n",
    "movie_comments['word_sequence'] = movie_comments.comment.apply(remove_char)\n",
    "\n",
    "movie_comments['word_sequence'] = movie_comments.word_sequence.apply(t2s)\n",
    "\n",
    "movie_comments['word_sequence'] = movie_comments.word_sequence.apply(cut)\n",
    "\n",
    "movie_comments.to_pickle('./movie_comments_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./movie_comments_data.pkl','rb') as f:\n",
    "    movie_comments = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter()\n",
    "for s in movie_comments['word_sequence']:\n",
    "    for w in s.split():\n",
    "        word_count[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'吴京': 279,\n",
       "         '意淫': 281,\n",
       "         '到': 10373,\n",
       "         '了': 102381,\n",
       "         '脑残': 319,\n",
       "         '的': 328466,\n",
       "         '地步': 197,\n",
       "         '，': 353805,\n",
       "         '看': 34250,\n",
       "         '恶心': 943,\n",
       "         '想': 7474,\n",
       "         '吐': 574,\n",
       "         '首映礼': 42,\n",
       "         '。': 219514,\n",
       "         '太': 13043,\n",
       "         '恐怖': 599,\n",
       "         '这个': 10282,\n",
       "         '电影': 34614,\n",
       "         '不讲道理': 8,\n",
       "         '完全': 4152,\n",
       "         '就是': 14015,\n",
       "         '在': 31161,\n",
       "         '实现': 270,\n",
       "         '他': 10663,\n",
       "         '小': 6649,\n",
       "         '粉红': 39,\n",
       "         '英雄': 1706,\n",
       "         '梦': 885,\n",
       "         '各种': 3143,\n",
       "         '装备': 83,\n",
       "         '轮番': 21,\n",
       "         '上场': 17,\n",
       "         '视': 29,\n",
       "         '物理': 63,\n",
       "         '逻辑': 1417,\n",
       "         '于': 1783,\n",
       "         '不顾': 57,\n",
       "         '不得不': 670,\n",
       "         '说': 11120,\n",
       "         '有钱': 205,\n",
       "         '真': 5188,\n",
       "         '好': 23131,\n",
       "         '随意': 170,\n",
       "         '胡闹': 45,\n",
       "         '炒作': 70,\n",
       "         '水平': 819,\n",
       "         '不输': 48,\n",
       "         '冯小刚': 266,\n",
       "         '但小刚': 1,\n",
       "         '至少': 912,\n",
       "         '不会': 2683,\n",
       "         '用': 3923,\n",
       "         '主旋律': 928,\n",
       "         '来': 5261,\n",
       "         '…': 26199,\n",
       "         '让': 13702,\n",
       "         '人': 24152,\n",
       "         '不': 28538,\n",
       "         '舒服': 611,\n",
       "         '为了': 3509,\n",
       "         '而': 6533,\n",
       "         '煽情': 1186,\n",
       "         '觉得': 8879,\n",
       "         '是': 72724,\n",
       "         '个': 6557,\n",
       "         '大': 5902,\n",
       "         '做作': 839,\n",
       "         '、': 18329,\n",
       "         '谎言': 269,\n",
       "         '家': 569,\n",
       "         '（': 4911,\n",
       "         '.': 14403,\n",
       "         '更新': 87,\n",
       "         '）': 4618,\n",
       "         '片子': 9594,\n",
       "         '整体': 1410,\n",
       "         '不如': 1806,\n",
       "         '湄公河': 57,\n",
       "         '行动': 187,\n",
       "         '不够': 1820,\n",
       "         '流畅': 702,\n",
       "         '编剧': 2274,\n",
       "         '有毒': 38,\n",
       "         '台词': 2370,\n",
       "         '尴尬': 1677,\n",
       "         '；': 5322,\n",
       "         '刻意': 1042,\n",
       "         '显得': 1098,\n",
       "         '如此': 2686,\n",
       "         '不合时宜': 33,\n",
       "         '又': 11553,\n",
       "         '多余': 311,\n",
       "         '凭良心说': 3,\n",
       "         '看到': 5459,\n",
       "         '不像': 277,\n",
       "         '《': 12004,\n",
       "         '战狼': 36,\n",
       "         '》': 12003,\n",
       "         '续集': 710,\n",
       "         '完虐': 7,\n",
       "         '中二得': 5,\n",
       "         '很': 34752,\n",
       "         '“': 11369,\n",
       "         '犯': 181,\n",
       "         '我': 50062,\n",
       "         '中华': 55,\n",
       "         '者': 548,\n",
       "         '虽远必': 22,\n",
       "         '诛': 33,\n",
       "         '”': 11160,\n",
       "         '比': 6098,\n",
       "         '这句': 231,\n",
       "         '话': 1758,\n",
       "         '还要': 780,\n",
       "         '一百倍': 14,\n",
       "         '脑子': 295,\n",
       "         '东西': 2291,\n",
       "         '希望': 1962,\n",
       "         '们': 2487,\n",
       "         '都': 36327,\n",
       "         '能': 9626,\n",
       "         '有': 27739,\n",
       "         '三星': 2055,\n",
       "         '半': 2053,\n",
       "         '实打实': 26,\n",
       "         '分': 2812,\n",
       "         '第一集': 211,\n",
       "         '爱国': 136,\n",
       "         '内部': 115,\n",
       "         '做': 4429,\n",
       "         '着': 6992,\n",
       "         '置换': 18,\n",
       "         '与': 9532,\n",
       "         '较劲': 17,\n",
       "         '但': 15535,\n",
       "         '第二集': 69,\n",
       "         '才': 4959,\n",
       "         '真正': 1484,\n",
       "         '显露': 17,\n",
       "         '野心': 262,\n",
       "         '终于': 1777,\n",
       "         '抛弃': 225,\n",
       "         '李忠志': 2,\n",
       "         '新增': 7,\n",
       "         '外来': 28,\n",
       "         '班底': 73,\n",
       "         '硬件': 24,\n",
       "         '实力': 194,\n",
       "         '机会': 386,\n",
       "         '和': 31337,\n",
       "         '国际': 208,\n",
       "         '接轨': 7,\n",
       "         '开篇': 193,\n",
       "         '水下': 33,\n",
       "         '长镜头': 689,\n",
       "         '诸如': 40,\n",
       "         '铁丝网': 3,\n",
       "         '拦截': 9,\n",
       "         '弹头': 8,\n",
       "         '细节': 2268,\n",
       "         '设计': 1177,\n",
       "         '国产': 1119,\n",
       "         '动作片': 1111,\n",
       "         '重新': 452,\n",
       "         '封顶': 4,\n",
       "         '理念': 113,\n",
       "         '上': 10113,\n",
       "         '它': 3303,\n",
       "         '甚至': 1278,\n",
       "         '做到': 533,\n",
       "         '绣春刀': 37,\n",
       "         '最': 8612,\n",
       "         '想做到': 5,\n",
       "         '那': 7414,\n",
       "         '部分': 1952,\n",
       "         '惊险': 107,\n",
       "         '大气': 184,\n",
       "         '引人入胜': 103,\n",
       "         '结合': 423,\n",
       "         '不俗': 76,\n",
       "         '快': 1200,\n",
       "         '剪下': 2,\n",
       "         '真刀真枪': 4,\n",
       "         '不禁': 137,\n",
       "         '热血沸腾': 225,\n",
       "         '特别': 2699,\n",
       "         '弹簧床': 3,\n",
       "         '架': 57,\n",
       "         '挡': 73,\n",
       "         '炸弹': 100,\n",
       "         '空手': 9,\n",
       "         '接': 330,\n",
       "         '碎玻璃': 4,\n",
       "         '弹匣': 2,\n",
       "         '割喉': 11,\n",
       "         '等': 1771,\n",
       "         '帅': 1045,\n",
       "         '得': 10017,\n",
       "         '飞起': 39,\n",
       "         '！': 55075,\n",
       "         '就算': 902,\n",
       "         '前半段': 588,\n",
       "         '铺垫': 587,\n",
       "         '节奏': 3517,\n",
       "         '散漫': 64,\n",
       "         '主角': 2141,\n",
       "         '光环': 217,\n",
       "         '开太大': 3,\n",
       "         '也': 32070,\n",
       "         '不怕': 135,\n",
       "         '作为': 2412,\n",
       "         '一个': 17829,\n",
       "         '中国': 3582,\n",
       "         '两个': 2953,\n",
       "         '小时': 1807,\n",
       "         '弥漫着': 37,\n",
       "         '强大': 694,\n",
       "         '不可': 516,\n",
       "         '侵犯': 24,\n",
       "         '氛围': 559,\n",
       "         '还是': 16858,\n",
       "         '那颗': 47,\n",
       "         '民族': 326,\n",
       "         '自豪': 24,\n",
       "         '心': 1091,\n",
       "         '砰砰': 53,\n",
       "         '砰': 58,\n",
       "         '跳个': 1,\n",
       "         '不停': 436,\n",
       "         '/': 6322,\n",
       "         '冷峰': 1,\n",
       "         '这部': 7641,\n",
       "         '里': 7888,\n",
       "         '即': 375,\n",
       "         '像': 5727,\n",
       "         '成龙': 885,\n",
       "         '像杰': 2,\n",
       "         '森斯坦': 150,\n",
       "         '森': 157,\n",
       "         '体制': 213,\n",
       "         '外': 501,\n",
       "         '同': 573,\n",
       "         '类型': 1511,\n",
       "         '总是': 1988,\n",
       "         '代表': 471,\n",
       "         '个人': 1735,\n",
       "         '无能': 185,\n",
       "         '政府': 244,\n",
       "         '需要': 1821,\n",
       "         '求助于': 4,\n",
       "         '这些': 1479,\n",
       "         '才能': 836,\n",
       "         '解决': 280,\n",
       "         '难题': 53,\n",
       "         '体现': 435,\n",
       "         '价值': 334,\n",
       "         '所以': 2768,\n",
       "         '照抄': 24,\n",
       "         '这种': 6498,\n",
       "         '模式': 481,\n",
       "         '实际上': 187,\n",
       "         '问题': 2398,\n",
       "         '我们': 6039,\n",
       "         '以前': 1095,\n",
       "         '嘲笑': 88,\n",
       "         '英雄主义': 283,\n",
       "         '却': 6254,\n",
       "         '没想到': 937,\n",
       "         '捆绑': 34,\n",
       "         '爱国主义': 87,\n",
       "         '全能': 17,\n",
       "         '战士': 112,\n",
       "         '更加': 722,\n",
       "         '难以': 482,\n",
       "         '下咽': 11,\n",
       "         '多': 9263,\n",
       "         '无脑': 258,\n",
       "         '信': 223,\n",
       "         '戏': 3726,\n",
       "         '对': 10153,\n",
       "         '吴京路': 1,\n",
       "         '转粉': 31,\n",
       "         '最后': 9967,\n",
       "         '彩蛋': 561,\n",
       "         '没有': 14757,\n",
       "         '理由': 375,\n",
       "         '期待': 1442,\n",
       "         '下': 3547,\n",
       "         '一部': 9687,\n",
       "         '假': 656,\n",
       "         '嗨': 215,\n",
       "         '几处': 175,\n",
       "         '情节': 3993,\n",
       "         '设置': 521,\n",
       "         '过于': 1024,\n",
       "         '彰显': 67,\n",
       "         '国家': 828,\n",
       "         '自豪感': 5,\n",
       "         '稍显': 201,\n",
       "         '突兀': 390,\n",
       "         '爽片': 23,\n",
       "         '打戏': 355,\n",
       "         '挺燃': 12,\n",
       "         '但是': 7721,\n",
       "         '故事': 15005,\n",
       "         '一般': 3549,\n",
       "         '达康': 13,\n",
       "         '书记': 19,\n",
       "         '合适': 297,\n",
       "         '角色': 3732,\n",
       "         '赵': 45,\n",
       "         '东来': 11,\n",
       "         '倒': 1606,\n",
       "         '张瀚': 6,\n",
       "         '太太': 203,\n",
       "         '太违': 15,\n",
       "         '分钟': 1825,\n",
       "         '穿越': 530,\n",
       "         '回': 401,\n",
       "         '偶像剧': 113,\n",
       "         '：': 6359,\n",
       "         '接到': 24,\n",
       "         '非洲': 134,\n",
       "         '卧底': 189,\n",
       "         '冷锋': 14,\n",
       "         '报告': 26,\n",
       "         '丁义珍': 6,\n",
       "         '现在': 3750,\n",
       "         '请求': 13,\n",
       "         '抓捕': 8,\n",
       "         '李达康': 4,\n",
       "         '这件': 111,\n",
       "         '事先': 24,\n",
       "         '不要': 2510,\n",
       "         '声张': 4,\n",
       "         '别': 732,\n",
       "         '省厅': 3,\n",
       "         '知道': 5386,\n",
       "         '就': 25693,\n",
       "         '你': 17210,\n",
       "         '一起': 2807,\n",
       "         '去': 7304,\n",
       "         '加上': 901,\n",
       "         '同志': 267,\n",
       "         '三人': 130,\n",
       "         '逮捕': 12,\n",
       "         '这次': 940,\n",
       "         '行': 1015,\n",
       "         '叫': 2031,\n",
       "         '吧': 10758,\n",
       "         '拍': 8200,\n",
       "         '喜剧': 2736,\n",
       "         '整个': 1785,\n",
       "         '感觉': 8017,\n",
       "         '挺': 6253,\n",
       "         '搞笑': 2400,\n",
       "         '这么': 6949,\n",
       "         '打': 3778,\n",
       "         '过': 3585,\n",
       "         '徐晓冬': 1,\n",
       "         '么': 3598,\n",
       "         '？': 19222,\n",
       "         '心往': 3,\n",
       "         '一处': 76,\n",
       "         '劲往': 3,\n",
       "         '使': 490,\n",
       "         '梦想': 1196,\n",
       "         '看吧': 159,\n",
       "         '第一部': 2316,\n",
       "         '好太多': 92,\n",
       "         '谢谢': 239,\n",
       "         '美队': 126,\n",
       "         '动作': 3388,\n",
       "         '指导': 121,\n",
       "         '这': 17339,\n",
       "         '火': 180,\n",
       "         '没见识': 5,\n",
       "         '开头': 1341,\n",
       "         '长': 1109,\n",
       "         '对决': 215,\n",
       "         '戏可算': 1,\n",
       "         '华语': 297,\n",
       "         '顶尖': 29,\n",
       "         '存在': 1295,\n",
       "         '驱逐舰': 4,\n",
       "         '导弹': 25,\n",
       "         '坦克': 215,\n",
       "         '商业片': 548,\n",
       "         '狂用': 1,\n",
       "         '镜头': 4296,\n",
       "         '运用': 397,\n",
       "         '笑': 3755,\n",
       "         '点': 3691,\n",
       "         '插入': 85,\n",
       "         '好莱坞': 1246,\n",
       "         '爆米花': 567,\n",
       "         '不功': 27,\n",
       "         '不过': 6527,\n",
       "         '从头': 338,\n",
       "         '打到': 69,\n",
       "         '尾': 340,\n",
       "         '拼': 312,\n",
       "         '虽然': 5822,\n",
       "         '有略': 4,\n",
       "         '乱': 690,\n",
       "         '时': 2942,\n",
       "         '因为': 4086,\n",
       "         '没': 11136,\n",
       "         '啥': 1874,\n",
       "         '期望值': 56,\n",
       "         '被': 9947,\n",
       "         '吓了一跳': 10,\n",
       "         '吴刚': 10,\n",
       "         '谦和': 6,\n",
       "         '丁海峰': 1,\n",
       "         '老': 3022,\n",
       "         '三位': 111,\n",
       "         '炖': 43,\n",
       "         '烂熟': 8,\n",
       "         '牛筋': 1,\n",
       "         '嚼': 51,\n",
       "         '用心': 635,\n",
       "         '啊': 20872,\n",
       "         '导演': 8669,\n",
       "         '小看': 21,\n",
       "         '确实': 2168,\n",
       "         '下功夫': 27,\n",
       "         '拉': 404,\n",
       "         '借鉴': 176,\n",
       "         '至于': 474,\n",
       "         '大家': 1361,\n",
       "         '比较': 3307,\n",
       "         '反感': 144,\n",
       "         '情绪': 1075,\n",
       "         '那些': 2407,\n",
       "         '桥段': 954,\n",
       "         '必备': 57,\n",
       "         '稍微': 405,\n",
       "         '一点': 2989,\n",
       "         '还': 17880,\n",
       "         '可以': 8972,\n",
       "         '接受': 925,\n",
       "         '最好': 1951,\n",
       "         '地方': 2173,\n",
       "         '掌握': 151,\n",
       "         '张弛': 68,\n",
       "         '有度': 62,\n",
       "         '这点': 283,\n",
       "         '难得': 882,\n",
       "         '一直': 3317,\n",
       "         '脑子里': 62,\n",
       "         '回响': 39,\n",
       "         '片头': 357,\n",
       "         '海里': 23,\n",
       "         '那场': 494,\n",
       "         '戏看': 22,\n",
       "         '完': 5015,\n",
       "         '呆': 248,\n",
       "         '下去': 878,\n",
       "         '太假': 207,\n",
       "         '提前': 195,\n",
       "         '离场': 130,\n",
       "         '好看': 8060,\n",
       "         '演技': 5630,\n",
       "         '棒呆': 34,\n",
       "         '符合': 413,\n",
       "         '反而': 828,\n",
       "         '更': 6120,\n",
       "         '差': 1706,\n",
       "         '这一': 125,\n",
       "         '放之四海而皆准': 3,\n",
       "         '规律': 30,\n",
       "         '场面': 2288,\n",
       "         '越做越': 10,\n",
       "         '然而': 793,\n",
       "         '伴随': 124,\n",
       "         '特效': 1979,\n",
       "         '升级': 123,\n",
       "         '叙事': 2095,\n",
       "         '变得': 605,\n",
       "         '非常': 4862,\n",
       "         '凌乱': 213,\n",
       "         '格局': 339,\n",
       "         '颇': 391,\n",
       "         '拍成': 446,\n",
       "         '黑鹰坠落': 30,\n",
       "         '结果': 1813,\n",
       "         '撑死': 16,\n",
       "         '最多': 97,\n",
       "         '只是': 4107,\n",
       "         '官方': 66,\n",
       "         '版': 2157,\n",
       "         '敢死队': 64,\n",
       "         '但论': 8,\n",
       "         '自我': 769,\n",
       "         '角色定位': 15,\n",
       "         '能力': 616,\n",
       "         '远': 381,\n",
       "         '如同': 318,\n",
       "         '演员': 5512,\n",
       "         '出身': 113,\n",
       "         '甄子丹': 389,\n",
       "         '喜欢': 13959,\n",
       "         '不是': 8787,\n",
       "         '装傻': 30,\n",
       "         '真傻': 19,\n",
       "         '要不是': 329,\n",
       "         '真的': 7923,\n",
       "         '别的': 445,\n",
       "         '可': 2097,\n",
       "         '肯定': 756,\n",
       "         '选': 321,\n",
       "         '直男癌': 59,\n",
       "         '令人发指': 105,\n",
       "         '所有': 2058,\n",
       "         '剧情': 11620,\n",
       "         '走向': 374,\n",
       "         '九十年代': 112,\n",
       "         '那套': 56,\n",
       "         '照搬': 101,\n",
       "         '审美': 269,\n",
       "         '事儿': 324,\n",
       "         '一时半会儿': 2,\n",
       "         '培养': 50,\n",
       "         '出来': 3402,\n",
       "         '整部': 1077,\n",
       "         '延续': 368,\n",
       "         '风格': 2812,\n",
       "         '热血': 762,\n",
       "         '比来': 1,\n",
       "         '要': 8115,\n",
       "         '不错': 10613,\n",
       "         '适合': 1841,\n",
       "         '演': 3457,\n",
       "         '军人': 81,\n",
       "         '之前': 1517,\n",
       "         '片段': 555,\n",
       "         '念': 131,\n",
       "         '劲儿': 131,\n",
       "         '来说': 1554,\n",
       "         '张翰太违': 1,\n",
       "         '一': 3708,\n",
       "         '一股': 327,\n",
       "         '雷阵雨': 3,\n",
       "         '画风': 450,\n",
       "         '目瞪狗': 2,\n",
       "         '瘠薄': 3,\n",
       "         '人牛': 5,\n",
       "         '硬道理': 18,\n",
       "         '隔壁': 128,\n",
       "         '建军': 6,\n",
       "         '大爷': 196,\n",
       "         '你们': 1877,\n",
       "         '场景': 1666,\n",
       "         '战斗': 355,\n",
       "         '全线': 14,\n",
       "         '打斗': 1162,\n",
       "         '游走': 49,\n",
       "         '审查': 133,\n",
       "         '红线': 12,\n",
       "         '边界': 27,\n",
       "         '政治': 1053,\n",
       "         '安全': 124,\n",
       "         '缝隙': 17,\n",
       "         '部': 720,\n",
       "         '极具': 157,\n",
       "         '煽动': 33,\n",
       "         '大片': 1043,\n",
       "         '制作': 1223,\n",
       "         '精良': 215,\n",
       "         '影片': 4879,\n",
       "         '请': 958,\n",
       "         '多来': 7,\n",
       "         '胶卷': 18,\n",
       "         '挺差': 9,\n",
       "         '过度': 269,\n",
       "         '部队': 81,\n",
       "         '没太多': 38,\n",
       "         '展示': 353,\n",
       "         '死去': 247,\n",
       "         '反正': 625,\n",
       "         '吸引': 807,\n",
       "         '冲': 253,\n",
       "         '为什么': 3237,\n",
       "         '鄙视': 87,\n",
       "         '敢': 350,\n",
       "         '开拓': 23,\n",
       "         '允许': 69,\n",
       "         '他们': 3131,\n",
       "         '再': 5342,\n",
       "         '直到': 337,\n",
       "         '更好': 1000,\n",
       "         '拍出': 576,\n",
       "         '棒': 1010,\n",
       "         '出彩': 1005,\n",
       "         '呢': 4505,\n",
       "         '火爆': 196,\n",
       "         '本片': 1954,\n",
       "         '必将': 38,\n",
       "         '燃爆': 59,\n",
       "         '暑期': 54,\n",
       "         '厉害': 851,\n",
       "         '身为': 125,\n",
       "         '武打': 320,\n",
       "         '高标准': 3,\n",
       "         '枪战': 696,\n",
       "         '为': 4910,\n",
       "         '点赞': 112,\n",
       "         '热血男儿': 2,\n",
       "         '荷尔蒙': 159,\n",
       "         '爆发': 407,\n",
       "         '给': 10403,\n",
       "         '星': 2239,\n",
       "         '血战': 39,\n",
       "         '钢锯': 19,\n",
       "         '岭': 34,\n",
       "         '会': 7743,\n",
       "         '歌颂': 74,\n",
       "         '宗教': 719,\n",
       "         '情怀': 962,\n",
       "         '超越': 579,\n",
       "         '政权': 33,\n",
       "         '当': 2871,\n",
       "         '只': 3341,\n",
       "         '明显': 1200,\n",
       "         '低': 753,\n",
       "         '层次': 173,\n",
       "         '充满': 1393,\n",
       "         '现实': 2248,\n",
       "         '乃至': 76,\n",
       "         '投机': 29,\n",
       "         '考量': 29,\n",
       "         '高下': 49,\n",
       "         '立': 91,\n",
       "         '见': 744,\n",
       "         '请问': 99,\n",
       "         '吴京脑': 3,\n",
       "         '残': 142,\n",
       "         '火箭炮': 6,\n",
       "         '吗': 4971,\n",
       "         '傲气': 6,\n",
       "         '雄鹰': 3,\n",
       "         '第一': 532,\n",
       "         '滴血': 26,\n",
       "         '算是': 1689,\n",
       "         '国内': 510,\n",
       "         '片': 6649,\n",
       "         '准': 48,\n",
       "         '钱': 942,\n",
       "         '花': 437,\n",
       "         '有效': 62,\n",
       "         '气魄': 22,\n",
       "         '创作': 266,\n",
       "         '足够': 607,\n",
       "         '真诚': 293,\n",
       "         '人物': 3996,\n",
       "         '连': 1644,\n",
       "         '张翰': 38,\n",
       "         '可爱': 3116,\n",
       "         '如果': 3615,\n",
       "         '当年': 1479,\n",
       "         '那样': 1074,\n",
       "         '一时': 103,\n",
       "         '膨胀': 34,\n",
       "         '银幕': 474,\n",
       "         '独占': 5,\n",
       "         '聚光灯': 9,\n",
       "         '走': 1813,\n",
       "         '扪心自问': 8,\n",
       "         '没法': 383,\n",
       "         '评价': 589,\n",
       "         '全片': 1218,\n",
       "         '靠': 1179,\n",
       "         '戏撑': 2,\n",
       "         '文戏': 309,\n",
       "         '扯淡': 253,\n",
       "         '女主角': 1429,\n",
       "         '毫无': 1353,\n",
       "         '必要': 476,\n",
       "         '只要': 716,\n",
       "         '开挂': 105,\n",
       "         '牛': 1633,\n",
       "         '逼': 3647,\n",
       "         '之处': 119,\n",
       "         '在于': 671,\n",
       "         '透露': 122,\n",
       "         '极': 239,\n",
       "         '强烈': 550,\n",
       "         '意识形态': 117,\n",
       "         '枷锁': 32,\n",
       "         '祖国': 67,\n",
       "         '面前': 408,\n",
       "         '一切': 1961,\n",
       "         '反动派': 5,\n",
       "         '纸老虎': 13,\n",
       "         '人开': 3,\n",
       "         '挂': 276,\n",
       "         '团灭': 13,\n",
       "         '合情合理': 33,\n",
       "         '两星': 745,\n",
       "         '鼓励': 469,\n",
       "         '其他': 1757,\n",
       "         '一般般': 484,\n",
       "         '看点': 515,\n",
       "         '有点': 7674,\n",
       "         '手接': 2,\n",
       "         '哈哈哈': 1637,\n",
       "         '从': 4309,\n",
       "         '之后': 2323,\n",
       "         '炸': 211,\n",
       "         '翻': 272,\n",
       "         '一下': 1818,\n",
       "         '四星': 1122,\n",
       "         '当时': 1066,\n",
       "         '其实': 5782,\n",
       "         '完成度': 199,\n",
       "         '接近': 310,\n",
       "         '每个': 2145,\n",
       "         '步骤': 8,\n",
       "         '顺滑': 6,\n",
       "         '任何': 1177,\n",
       "         '出人意料': 76,\n",
       "         '是因为': 882,\n",
       "         '看看': 1632,\n",
       "         '最近': 763,\n",
       "         '世界': 3435,\n",
       "         '抱歉': 76,\n",
       "         '影院': 788,\n",
       "         '燃': 183,\n",
       "         '起来': 2043,\n",
       "         '魔幻': 378,\n",
       "         '当然': 1233,\n",
       "         '强拆': 38,\n",
       "         '现实感': 16,\n",
       "         '一幕': 481,\n",
       "         '开场': 457,\n",
       "         '搏斗': 38,\n",
       "         '从来': 312,\n",
       "         '其它': 192,\n",
       "         '拍摄': 964,\n",
       "         '难度': 72,\n",
       "         '同时': 732,\n",
       "         '技能': 81,\n",
       "         '方面': 809,\n",
       "         '要求': 318,\n",
       "         '回来': 523,\n",
       "         '搜': 51,\n",
       "         '吴京会': 1,\n",
       "         '游泳': 37,\n",
       "         '潜水': 9,\n",
       "         '滑雪': 14,\n",
       "         '开': 517,\n",
       "         '飞机': 553,\n",
       "         '射击': 41,\n",
       "         '各项': 6,\n",
       "         '特意': 108,\n",
       "         '特种部队': 36,\n",
       "         '当过': 7,\n",
       "         '月': 460,\n",
       "         '兵': 46,\n",
       "         '佩服': 344,\n",
       "         '这样': 6673,\n",
       "         '星半': 174,\n",
       "         '结束': 893,\n",
       "         '掌声': 52,\n",
       "         '出现': 1635,\n",
       "         '近期': 118,\n",
       "         '少见': 111,\n",
       "         '一粒': 17,\n",
       "         '大补丸': 1,\n",
       "         '有人': 940,\n",
       "         '吃': 1258,\n",
       "         '开心': 775,\n",
       "         '补大': 1,\n",
       "         '从白': 1,\n",
       "         '黑': 1012,\n",
       "         '字幕': 770,\n",
       "         '展现': 668,\n",
       "         '超级': 1301,\n",
       "         '直': 233,\n",
       "         '男': 1876,\n",
       "         '糙': 67,\n",
       "         '猛': 132,\n",
       "         '媲美': 80,\n",
       "         '终结者': 69,\n",
       "         '无亮点': 65,\n",
       "         '张翰变': 1,\n",
       "         '谐星': 25,\n",
       "         '掌控': 194,\n",
       "         '逼近': 26,\n",
       "         '不住': 213,\n",
       "         '边缘': 168,\n",
       "         '带感': 178,\n",
       "         '拳拳': 105,\n",
       "         '肉': 293,\n",
       "         '超爽': 13,\n",
       "         '聪明': 349,\n",
       "         '鸡': 166,\n",
       "         '贼': 87,\n",
       "         '一面': 296,\n",
       "         '旗下': 9,\n",
       "         '呈现': 436,\n",
       "         '一出': 296,\n",
       "         '重工业': 4,\n",
       "         '娱乐': 507,\n",
       "         '调控': 9,\n",
       "         '说教': 316,\n",
       "         '比例': 43,\n",
       "         '尺度': 197,\n",
       "         '大众': 203,\n",
       "         '接纳': 26,\n",
       "         '把握': 368,\n",
       "         '微妙': 226,\n",
       "         '其中': 759,\n",
       "         '一些': 2104,\n",
       "         '奇侠': 9,\n",
       "         '化': 628,\n",
       "         '内容': 1015,\n",
       "         '比如': 613,\n",
       "         '玻璃碴': 2,\n",
       "         '子当': 1,\n",
       "         '飞镖': 14,\n",
       "         '杀敌': 10,\n",
       "         '一类': 108,\n",
       "         '只不过': 325,\n",
       "         '遮盖': 13,\n",
       "         '掉': 688,\n",
       "         '老爹': 81,\n",
       "         '演过': 74,\n",
       "         '美剧': 80,\n",
       "         '搏击': 76,\n",
       "         '王国': 47,\n",
       "         '力荐': 107,\n",
       "         '那部': 123,\n",
       "         '为啥': 477,\n",
       "         '奇异': 83,\n",
       "         '恩典': 2,\n",
       "         '配乐': 2810,\n",
       "         '画内': 1,\n",
       "         '男生': 186,\n",
       "         '的话': 1393,\n",
       "         '应该': 3298,\n",
       "         '刺激': 630,\n",
       "         '肾上腺素': 67,\n",
       "         '女生': 334,\n",
       "         '对龙小云': 1,\n",
       "         '感情': 1738,\n",
       "         '十分': 830,\n",
       "         '打动': 598,\n",
       "         '模仿': 570,\n",
       "         '许多': 551,\n",
       "         '怎么': 4816,\n",
       "         '玩': 1231,\n",
       "         '一股脑': 15,\n",
       "         '堆': 40,\n",
       "         '槽': 584,\n",
       "         '几位': 177,\n",
       "         '血厚到': 1,\n",
       "         '科幻': 699,\n",
       "         '级别': 187,\n",
       "         '重复': 359,\n",
       "         '满血': 15,\n",
       "         '红血': 1,\n",
       "         '中毒': 17,\n",
       "         '极速': 34,\n",
       "         '回血': 5,\n",
       "         '爆种': 3,\n",
       "         '打通': 14,\n",
       "         '全场': 345,\n",
       "         '...': 5709,\n",
       "         '太过': 616,\n",
       "         '投机取巧': 10,\n",
       "         '穿': 515,\n",
       "         '迈克尔': 84,\n",
       "         '贝都': 1,\n",
       "         '不受': 10,\n",
       "         '待见': 65,\n",
       "         '国片': 19,\n",
       "         '前仆后继': 9,\n",
       "         '爆炸': 371,\n",
       "         '假瞎燃': 1,\n",
       "         '没用': 136,\n",
       "         '女人': 2372,\n",
       "         '缺': 169,\n",
       "         '男人': 2387,\n",
       "         '征服': 75,\n",
       "         '吴京直': 1,\n",
       "         '男癌': 34,\n",
       "         '🇨': 1,\n",
       "         '🇳': 1,\n",
       "         '美国': 2622,\n",
       "         '不行': 1066,\n",
       "         '死': 3846,\n",
       "         '全都': 247,\n",
       "         '跳': 489,\n",
       "         '跟': 4141,\n",
       "         '跳墙': 3,\n",
       "         '一样': 4728,\n",
       "         '拯救': 678,\n",
       "         '国产片': 358,\n",
       "         '以': 1947,\n",
       "         '中印': 3,\n",
       "         '局势': 11,\n",
       "         '对比': 464,\n",
       "         '假想': 8,\n",
       "         '真是': 6452,\n",
       "         '讽刺': 765,\n",
       "         '谄媚': 19,\n",
       "         '军旅': 11,\n",
       "         '题材': 2764,\n",
       "         '质感': 370,\n",
       "         '燃到': 34,\n",
       "         '国外': 150,\n",
       "         '精彩': 2853,\n",
       "         '看着': 1895,\n",
       "         '有力': 225,\n",
       "         '必须': 698,\n",
       "         '安利': 43,\n",
       "         '一下张': 2,\n",
       "         '翰': 3,\n",
       "         '简直': 2429,\n",
       "         '承包': 27,\n",
       "         '笑点': 1117,\n",
       "         '量身定做': 34,\n",
       "         '彭于': 225,\n",
       "         '晏': 205,\n",
       "         '可演': 6,\n",
       "         '不来': 66,\n",
       "         '不少': 1113,\n",
       "         '漂移': 19,\n",
       "         '无人机': 41,\n",
       "         '突袭': 59,\n",
       "         '直升机': 68,\n",
       "         '坠露': 2,\n",
       "         '肉搏': 96,\n",
       "         '军舰': 13,\n",
       "         '发射': 18,\n",
       "         '叛乱': 4,\n",
       "         '国际化': 22,\n",
       "         '视角': 778,\n",
       "         '标配': 55,\n",
       "         '饰演': 260,\n",
       "         '深入人心': 49,\n",
       "         '搏命': 41,\n",
       "         '精神': 990,\n",
       "         '当下': 254,\n",
       "         '第三部': 309,\n",
       "         '好燃': 17,\n",
       "         '表白': 124,\n",
       "         '典型': 889,\n",
       "         '方式': 1254,\n",
       "         '每次': 638,\n",
       "         '猜': 553,\n",
       "         '没劲': 224,\n",
       "         '诶': 271,\n",
       "         '~': 21638,\n",
       "         '问': 421,\n",
       "         '王牌': 84,\n",
       "         '特工': 401,\n",
       "         '那么': 6596,\n",
       "         '杀人': 526,\n",
       "         '经过': 190,\n",
       "         '艺术': 696,\n",
       "         '处理': 1053,\n",
       "         '直接': 1102,\n",
       "         '删': 126,\n",
       "         '血腥': 585,\n",
       "         '屠杀': 63,\n",
       "         '赤裸裸': 113,\n",
       "         '大段': 149,\n",
       "         '正确': 300,\n",
       "         '庇衣': 1,\n",
       "         '意料之中': 88,\n",
       "         '意料之外': 67,\n",
       "         '惊喜': 1335,\n",
       "         '属于': 748,\n",
       "         '狼性': 4,\n",
       "         '军魂': 2,\n",
       "         '几个': 1927,\n",
       "         '网红拉': 1,\n",
       "         '弹弹琴': 2,\n",
       "         '大国': 36,\n",
       "         '气象': 12,\n",
       "         '满屏': 79,\n",
       "         '这才': 554,\n",
       "         '告诉': 936,\n",
       "         '吴': 104,\n",
       "         '迪塞尔': 58,\n",
       "         '如入无人之境': 4,\n",
       "         '亿': 75,\n",
       "         '大陆': 716,\n",
       "         '一刻': 195,\n",
       "         '集体': 273,\n",
       "         '勃起': 11,\n",
       "         '离开': 469,\n",
       "         '影厅': 27,\n",
       "         '屌丝': 381,\n",
       "         '同样': 867,\n",
       "         '开始': 3353,\n",
       "         '前': 2745,\n",
       "         ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 125794\n"
     ]
    }
   ],
   "source": [
    "print('Total words',len(word_count))\n",
    "\n",
    "vocab_size = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {}\n",
    "id_to_word[0] = '<PAD>'\n",
    "\n",
    "sort_word_fre = sorted(word_count,key=word_count.get,reverse=True)\n",
    "\n",
    "for i,w in enumerate(sort_word_fre[:vocab_size],1):id_to_word[i] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {w:i for i,w in id_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(word_sequence,maxlength):\n",
    "    pad_sequence = np.zeros((len(word_sequence),maxlength),dtype=np.int32)\n",
    "    for ri,row in enumerate(word_sequence):\n",
    "        for ci,w in enumerate(row.split()[:maxlength]):\n",
    "            pad_sequence[ri,ci] = word_to_id.get(w,0)\n",
    "    return pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = pad_sequence(movie_comments['word_sequence'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "star = movie_comments['star'].values\n",
    "star = star.astype(np.int32)\n",
    "sub_one = np.vectorize(lambda x:x-1)\n",
    "star = sub_one(star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,c = np.unique(star,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.09393332160584947\n",
      "1 0.10759711503896839\n",
      "2 0.2511415175874016\n",
      "3 0.3204968374035351\n",
      "4 0.22683120836424545\n"
     ]
    }
   ],
   "source": [
    "for s,n in zip(u,c):print(s,n/sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_star = np.eye(5,dtype=np.int32)[star]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_batches(datasets,labels,batch_size):\n",
    "    assert datasets.shape[0] == labels.shape[0]\n",
    "    batch_num = datasets.shape[0] // batch_size\n",
    "    datasets_cut = datasets[:batch_num*batch_size,:]\n",
    "    last_datasets = datasets[batch_num*batch_size:,:]\n",
    "    labels_cut = labels[:batch_num*batch_size,:]\n",
    "    last_labels = labels[batch_num*batch_size:,:]\n",
    "    \n",
    "    for i in range(batch_num+1):\n",
    "        if i == batch_num:\n",
    "            #yield last_datasets,last_labels\n",
    "            break\n",
    "        x = datasets_cut[i*batch_size:(i+1)*batch_size,:]\n",
    "        y = labels_cut[i*batch_size:(i+1)*batch_size,:]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX 构建神经网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q22:在没有预训练的词向量时候， keras 如何实现embedding操作，即如何依据一个单词的序列获得其向量表示？\n",
    "回答：\n",
    "```python\n",
    "max_feature = 200\n",
    "embed_size =300\n",
    "vocab_size = len(vocab_to_id)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,embed_size))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q23:在没有预训练的词向量时候， tensorflow 如何实现embedding操作，即如何依据一个单词的序列获得其向量表示？\n",
    "回答：\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "input_seq = tf.Placeholder(tf.int32,[None,max_feature])\n",
    "\n",
    "embedding = tf.Variable(tf.random_uniform((vocab_size,embed_size),-1,1))\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embedding,input_seq)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q24: 在有预先训练的词向量时候，keras和tensorflow又如何实现embeding操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "keras:\n",
    "```python\n",
    "pretrained_vec = ... # is a pretrained embedding numpy ndarray\n",
    "\n",
    "import keras\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input,Dense,Embedding\n",
    "\n",
    "embeddings_initializer = keras.initializers.Constant(value=pretrained_vec)\n",
    "\n",
    "inputs = Input(shape=(max_feature,))\n",
    "embed_layer = Embedding(vocab_size,embed_size,embeddings_initializer=embeddings_initializer)(inputs)\n",
    "embed_layer.trainable = False\n",
    "```\n",
    "\n",
    "tensorflow:\n",
    "```python\n",
    "pretrained_vec = ... # is a pretrained embedding numpy ndarray\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "input_seq = tf.Placeholder(tf.int32,[None,max_feature])\n",
    "\n",
    "embedding = tf.get_variable(name=\"embedding\", shape=pretrained_vec.shape, initializer=tf.constant_initializer(pretrained_vec), trainable=False)\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embedding,input_seq)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q25： 基于上文进行的数据预处理，使用keras和tensorflow如何构建神经网络模型？请提供关键代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答： keras模型构建的关键代码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：tensorflow模型构建的关键代码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  X 使用keras的history观察loss以及accuracy的变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q26: keras如何观察模型的loss变化以及准确率的变化，请列出关键代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "```python\n",
    "history = sentiment_model.fit(train_x,train_y,batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(val_x,val_y),\n",
    "                             verbose=1)\n",
    "\n",
    "print(history.history.keys())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q27: 请使用matplotlib画出loss变化的趋势；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-ceaedbc572b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XI 使用tensorflow tensorboard观察loss, accuracy的变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q28: tensorflow如何观察模型的loss变化以及准确率的变化， tensor board 如何使用？ 请列出关键代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q29: 试着点击tensor board的不同按钮 观察图像的变化； 试着给tensorflow board机制 写入不同时候训练的模型时候，给模型取不同的名字，观察tensor board的图像变化；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XII 观察熟悉RNN的两种变体的原理和方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q30:试着改进RNN，使用LSTM， GRU 进行模型的改动， 观察训练结果(loss和accuracy)的变化， 你观察到了什么变化？ 如何解释？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (209195, 200)\n",
      "val_x shape: (26149, 200)\n",
      "test_x shape: (26150, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sequence,onehot_star = shuffle(sequence,onehot_star,random_state=42)\n",
    "\n",
    "split_idx = int(len(sequence)*0.8)\n",
    "\n",
    "train_x, val_x = sequence[:split_idx], sequence[split_idx:]\n",
    "train_y, val_y = onehot_star[:split_idx], onehot_star[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('val_x shape:', val_x.shape)\n",
    "print('test_x shape:', test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature = 200\n",
    "embed_size = 300\n",
    "lstm_size = 32\n",
    "lstm_layers = 2\n",
    "dropout_rate = 0.5\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./cc.zh.300.vec')\n",
    "\n",
    "count_not_in_pretrain = 0\n",
    "word_not_in_pretrain = []\n",
    "np.random.seed(42)\n",
    "pretrianed_wv = np.random.uniform(-1,1,(vocab_size+1,embed_size))\n",
    "for word,idx in word_to_id.items():\n",
    "    if word not in word_vectors:\n",
    "        count_not_in_pretrain += 1\n",
    "        word_not_in_pretrain.append(word)\n",
    "        continue\n",
    "    pretrianed_wv[idx] = word_vectors[word]\n",
    "\n",
    "print(pretrianed_wv.shape)\n",
    "\n",
    "del word_vectors\n",
    "\n",
    "pretrianed_wv.dump('./pretrained_wv.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrianed_wv = np.load('./pretrained_wv.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(word_not_in_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('word not in pretrained vocab: %.2f' % (count_not_in_pretrain/vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for sentimental analysis(construct model keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input,Embedding,Dense,CuDNNLSTM,LSTM,Bidirectional,Lambda,dot,Activation,concatenate,Dropout,Masking\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import adam\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', min_delta=0.00005, patience=3, verbose=0,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_blocks(hidden_states,max_feature=200,attention_size=128):\n",
    "    hidden_size = int(hidden_states.shape[2])\n",
    "    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
    "    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
    "    score = dot([score_first_part, h_t], [2, 1], name='attention_score')\n",
    "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
    "    context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')\n",
    "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
    "    attention_vector = Dense(attention_size, use_bias=False, activation='tanh',name='attention_vector')(pre_activation)\n",
    "\n",
    "    return attention_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(max_feature,))\n",
    "\n",
    "\n",
    "weights = Constant(value=pretrianed_wv)\n",
    "embed_layer = Embedding(vocab_size+1,embed_size,embeddings_initializer=weights)\n",
    "embed_layer.trainable = True\n",
    "\n",
    "embed = embed_layer(inputs)\n",
    "#embed = Masking(mask_value=pretrianed_wv[0])(embed)\n",
    "\n",
    "biLSTM_layer1 = Bidirectional(CuDNNLSTM(lstm_size,return_sequences=True,kernel_regularizer=l2(0.002),recurrent_regularizer=l2(0.002)))(embed)\n",
    "biLSTM_layer1 = Dropout(dropout_rate)(biLSTM_layer1)\n",
    "\n",
    "biLSTM_layer2 = Bidirectional(CuDNNLSTM(lstm_size,return_sequences=True,kernel_regularizer=l2(0.002),recurrent_regularizer=l2(0.002)))(biLSTM_layer1)\n",
    "biLSTM_layer2 = Dropout(dropout_rate)(biLSTM_layer2)\n",
    "\n",
    "\n",
    "attention_mul = attention_3d_blocks(biLSTM_layer2)\n",
    "\n",
    "out_ = Dense(5, activation='softmax')(attention_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     18000300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200, 64)      85504       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200, 64)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200, 64)      25088       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200, 64)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_score_vec (Dense)     (None, 200, 64)      4096        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "last_hidden_state (Lambda)      (None, 64)           0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_score (Dot)           (None, 200)          0           attention_score_vec[0][0]        \n",
      "                                                                 last_hidden_state[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "attention_weight (Activation)   (None, 200)          0           attention_score[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "context_vector (Dot)            (None, 64)           0           dropout_2[0][0]                  \n",
      "                                                                 attention_weight[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_output (Concatenate)  (None, 128)          0           context_vector[0][0]             \n",
      "                                                                 last_hidden_state[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "attention_vector (Dense)        (None, 128)          16384       attention_output[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            645         attention_vector[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 18,132,017\n",
      "Trainable params: 18,132,017\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = Model(inputs=inputs,outputs=out_)\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=opt,\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 209195 samples, validate on 26149 samples\n",
      "Epoch 1/50\n",
      "209195/209195 [==============================] - 1930s 9ms/step - loss: 1.3375 - acc: 0.4383 - val_loss: 1.2284 - val_acc: 0.4650\n",
      "Epoch 2/50\n",
      "209195/209195 [==============================] - 1928s 9ms/step - loss: 1.1549 - acc: 0.5077 - val_loss: 1.2117 - val_acc: 0.4825\n",
      "Epoch 3/50\n",
      "209195/209195 [==============================] - 1922s 9ms/step - loss: 1.0819 - acc: 0.5489 - val_loss: 1.2277 - val_acc: 0.4823\n",
      "Epoch 4/50\n",
      " 13184/209195 [>.............................] - ETA: 29:05 - loss: 0.9681 - acc: 0.6056"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-656a7cbac44e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                              \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                              verbose=1)\n\u001b[0m",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = sentiment_model.fit(train_x,train_y,batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                              callbacks=callbacks,\n",
    "                             validation_data=(val_x,val_y),\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26150/26150 [==============================] - 13s 507us/step\n",
      "Test score: 1.5843035610136977\n",
      "Test accuracy 0.5173996175931015\n"
     ]
    }
   ],
   "source": [
    "score,acc = sentiment_model.evaluate(test_x,test_y,\n",
    "                                    batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "\n",
    "print('Test accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.save_weights('./checkpoints/keras_biatten_w.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.layers import *\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcnn_inputs = Input(shape=(max_feature,))\n",
    "\n",
    "\n",
    "\n",
    "weights = Constant(value=pretrianed_wv)\n",
    "embed_layer = Embedding(vocab_size+1,embed_size,embeddings_initializer=weights)\n",
    "embed_layer.trainable = True\n",
    "\n",
    "embed = embed_layer(inputs)\n",
    "\n",
    "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(embed)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "bigram_branch = Dropout(dropout_rate)(bigram_branch)\n",
    "\n",
    "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(embed)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "trigram_branch = Dropout(dropout_rate)(trigram_branch)\n",
    "\n",
    "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(embed)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "fourgram_branch = Dropout(dropout_rate)(fourgram_branch)\n",
    "\n",
    "\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "outmerge = Dense(5,activation='softmax')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 200)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     18000300    masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 199, 100)     60100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 198, 100)     90100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 197, 100)     120100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 100)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100)          400         global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100)          400         global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100)          400         global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 300)          1200        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            1285        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 18,352,365\n",
      "Trainable params: 18,350,653\n",
      "Non-trainable params: 1,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tcnn_model = Model(inputs=tcnn_inputs,outputs=outmerge)\n",
    "tcnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcnn_model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=opt,\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 209195 samples, validate on 26149 samples\n",
      "Epoch 1/50\n",
      "209195/209195 [==============================] - 325s 2ms/step - loss: 1.3430 - acc: 0.4000 - val_loss: 3.7954 - val_acc: 0.3342\n",
      "Epoch 2/50\n",
      "209195/209195 [==============================] - 321s 2ms/step - loss: 1.1804 - acc: 0.4793 - val_loss: 2.0329 - val_acc: 0.2767\n",
      "Epoch 3/50\n",
      "209195/209195 [==============================] - 321s 2ms/step - loss: 1.0769 - acc: 0.5368 - val_loss: 2.9755 - val_acc: 0.2974\n",
      "Epoch 4/50\n",
      "209195/209195 [==============================] - 318s 2ms/step - loss: 0.9732 - acc: 0.5922 - val_loss: 5.2486 - val_acc: 0.3677\n",
      "Epoch 5/50\n",
      "209195/209195 [==============================] - 318s 2ms/step - loss: 0.8755 - acc: 0.6411 - val_loss: 1.6451 - val_acc: 0.4565\n",
      "Epoch 6/50\n",
      "209195/209195 [==============================] - 318s 2ms/step - loss: 0.7936 - acc: 0.6786 - val_loss: 1.6011 - val_acc: 0.4692\n",
      "Epoch 7/50\n",
      "209195/209195 [==============================] - 318s 2ms/step - loss: 0.7216 - acc: 0.7098 - val_loss: 1.3733 - val_acc: 0.5002\n",
      "Epoch 8/50\n",
      "209195/209195 [==============================] - 318s 2ms/step - loss: 0.6657 - acc: 0.7325 - val_loss: 1.3303 - val_acc: 0.5104\n",
      "Epoch 9/50\n",
      "114848/209195 [===============>..............] - ETA: 2:21 - loss: 0.6037 - acc: 0.7610"
     ]
    }
   ],
   "source": [
    "history_cnn = tcnn_model.fit(train_x,train_y,batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                              callbacks=callbacks,\n",
    "                             validation_data=(val_x,val_y),\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26150/26150 [==============================] - 4s 157us/step\n",
      "Test score: 1.9522584938501537\n",
      "Test accuracy 0.5313575525824016\n"
     ]
    }
   ],
   "source": [
    "score,acc = tcnn_model.evaluate(test_x,test_y,\n",
    "                                    batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "\n",
    "print('Test accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcnn_model.save_weights('./checkpoints/keras_tcnn_w.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for sentimental analysis(construct model tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_rate = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_graph = tf.Graph()\n",
    "\n",
    "with sentiment_graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None,None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None,None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sentiment_graph.as_default():\n",
    "    embedding = tf.get_variable(name='embedding',shape=pretrianed_wv.shape,\n",
    "                                initializer=tf.constant_initializer(pretrianed_wv),\n",
    "                               trainable=False)\n",
    "    embed_ = tf.nn.embedding_lookup(embedding,inputs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sentiment_graph.as_default():\n",
    "    def build_cell(lstm_size,keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size,keep_prob) for _ in range(lstm_layers)])\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    outputs , final_state = tf.nn.dynamic_rnn(cell,embed_,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sentiment_graph.as_default():\n",
    "    logits = tf.contrib.layers.fully_connected(outputs[:,-1],5,activation_fn=tf.sigmoid)\n",
    "    \n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=labels_))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sentiment_graph.as_default():\n",
    "    correct_pred = tf.equal(tf.argmax(predictions,1),tf.argmax(labels_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20 Iterarion:5 Train loss: 1.521 Accuracy:0.3340\n",
      "Epoch: 0/20 Iterarion:10 Train loss: 1.520 Accuracy:0.2891\n",
      "Epoch: 0/20 Iterarion:15 Train loss: 1.522 Accuracy:0.3164\n",
      "Epoch: 0/20 Iterarion:20 Train loss: 1.508 Accuracy:0.3379\n",
      "Epoch: 0/20 Iterarion:25 Train loss: 1.518 Accuracy:0.2871\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:30 Train loss: 1.505 Accuracy:0.3477\n",
      "Epoch: 0/20 Iterarion:35 Train loss: 1.497 Accuracy:0.3477\n",
      "Epoch: 0/20 Iterarion:40 Train loss: 1.496 Accuracy:0.3203\n",
      "Epoch: 0/20 Iterarion:45 Train loss: 1.512 Accuracy:0.3398\n",
      "Epoch: 0/20 Iterarion:50 Train loss: 1.504 Accuracy:0.3262\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:55 Train loss: 1.521 Accuracy:0.2969\n",
      "Epoch: 0/20 Iterarion:60 Train loss: 1.515 Accuracy:0.3223\n",
      "Epoch: 0/20 Iterarion:65 Train loss: 1.498 Accuracy:0.3457\n",
      "Epoch: 0/20 Iterarion:70 Train loss: 1.536 Accuracy:0.2969\n",
      "Epoch: 0/20 Iterarion:75 Train loss: 1.518 Accuracy:0.3008\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:80 Train loss: 1.515 Accuracy:0.3164\n",
      "Epoch: 0/20 Iterarion:85 Train loss: 1.503 Accuracy:0.3262\n",
      "Epoch: 0/20 Iterarion:90 Train loss: 1.517 Accuracy:0.3359\n",
      "Epoch: 0/20 Iterarion:95 Train loss: 1.525 Accuracy:0.3262\n",
      "Epoch: 0/20 Iterarion:100 Train loss: 1.501 Accuracy:0.2910\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:105 Train loss: 1.539 Accuracy:0.3145\n",
      "Epoch: 0/20 Iterarion:110 Train loss: 1.521 Accuracy:0.3242\n",
      "Epoch: 0/20 Iterarion:115 Train loss: 1.509 Accuracy:0.3125\n",
      "Epoch: 0/20 Iterarion:120 Train loss: 1.551 Accuracy:0.2988\n",
      "Epoch: 0/20 Iterarion:125 Train loss: 1.506 Accuracy:0.3027\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:130 Train loss: 1.507 Accuracy:0.3281\n",
      "Epoch: 0/20 Iterarion:135 Train loss: 1.503 Accuracy:0.3125\n",
      "Epoch: 0/20 Iterarion:140 Train loss: 1.505 Accuracy:0.3066\n",
      "Epoch: 0/20 Iterarion:145 Train loss: 1.518 Accuracy:0.3125\n",
      "Epoch: 0/20 Iterarion:150 Train loss: 1.523 Accuracy:0.3164\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:155 Train loss: 1.489 Accuracy:0.3438\n",
      "Epoch: 0/20 Iterarion:160 Train loss: 1.492 Accuracy:0.3574\n",
      "Epoch: 0/20 Iterarion:165 Train loss: 1.512 Accuracy:0.2930\n",
      "Epoch: 0/20 Iterarion:170 Train loss: 1.523 Accuracy:0.2930\n",
      "Epoch: 0/20 Iterarion:175 Train loss: 1.520 Accuracy:0.3066\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:180 Train loss: 1.516 Accuracy:0.3125\n",
      "Epoch: 0/20 Iterarion:185 Train loss: 1.511 Accuracy:0.3184\n",
      "Epoch: 0/20 Iterarion:190 Train loss: 1.498 Accuracy:0.3184\n",
      "Epoch: 0/20 Iterarion:195 Train loss: 1.521 Accuracy:0.3125\n",
      "Epoch: 0/20 Iterarion:200 Train loss: 1.530 Accuracy:0.2969\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:205 Train loss: 1.530 Accuracy:0.3223\n",
      "Epoch: 0/20 Iterarion:210 Train loss: 1.516 Accuracy:0.2773\n",
      "Epoch: 0/20 Iterarion:215 Train loss: 1.538 Accuracy:0.2871\n",
      "Epoch: 0/20 Iterarion:220 Train loss: 1.517 Accuracy:0.3066\n",
      "Epoch: 0/20 Iterarion:225 Train loss: 1.526 Accuracy:0.2910\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:230 Train loss: 1.525 Accuracy:0.3008\n",
      "Epoch: 0/20 Iterarion:235 Train loss: 1.510 Accuracy:0.3242\n",
      "Epoch: 0/20 Iterarion:240 Train loss: 1.533 Accuracy:0.2988\n",
      "Epoch: 0/20 Iterarion:245 Train loss: 1.548 Accuracy:0.3203\n",
      "Epoch: 0/20 Iterarion:250 Train loss: 1.504 Accuracy:0.3340\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:255 Train loss: 1.555 Accuracy:0.2559\n",
      "Epoch: 0/20 Iterarion:260 Train loss: 1.510 Accuracy:0.2988\n",
      "Epoch: 0/20 Iterarion:265 Train loss: 1.538 Accuracy:0.2988\n",
      "Epoch: 0/20 Iterarion:270 Train loss: 1.526 Accuracy:0.3203\n",
      "Epoch: 0/20 Iterarion:275 Train loss: 1.523 Accuracy:0.3047\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:280 Train loss: 1.498 Accuracy:0.3008\n",
      "Epoch: 0/20 Iterarion:285 Train loss: 1.511 Accuracy:0.2969\n",
      "Epoch: 0/20 Iterarion:290 Train loss: 1.496 Accuracy:0.3340\n",
      "Epoch: 0/20 Iterarion:295 Train loss: 1.521 Accuracy:0.3516\n",
      "Epoch: 0/20 Iterarion:300 Train loss: 1.530 Accuracy:0.3066\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:305 Train loss: 1.506 Accuracy:0.3359\n",
      "Epoch: 0/20 Iterarion:310 Train loss: 1.498 Accuracy:0.3242\n",
      "Epoch: 0/20 Iterarion:315 Train loss: 1.516 Accuracy:0.3242\n",
      "Epoch: 0/20 Iterarion:320 Train loss: 1.519 Accuracy:0.3730\n",
      "Epoch: 0/20 Iterarion:325 Train loss: 1.520 Accuracy:0.3184\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:330 Train loss: 1.485 Accuracy:0.3379\n",
      "Epoch: 0/20 Iterarion:335 Train loss: 1.507 Accuracy:0.3105\n",
      "Epoch: 0/20 Iterarion:340 Train loss: 1.516 Accuracy:0.3066\n",
      "Epoch: 0/20 Iterarion:345 Train loss: 1.498 Accuracy:0.3223\n",
      "Epoch: 0/20 Iterarion:350 Train loss: 1.521 Accuracy:0.3125\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:355 Train loss: 1.513 Accuracy:0.2988\n",
      "Epoch: 0/20 Iterarion:360 Train loss: 1.500 Accuracy:0.2949\n",
      "Epoch: 0/20 Iterarion:365 Train loss: 1.499 Accuracy:0.3438\n",
      "Epoch: 0/20 Iterarion:370 Train loss: 1.529 Accuracy:0.3164\n",
      "Epoch: 0/20 Iterarion:375 Train loss: 1.513 Accuracy:0.3633\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:380 Train loss: 1.544 Accuracy:0.2949\n",
      "Epoch: 0/20 Iterarion:385 Train loss: 1.520 Accuracy:0.3184\n",
      "Epoch: 0/20 Iterarion:390 Train loss: 1.513 Accuracy:0.3301\n",
      "Epoch: 0/20 Iterarion:395 Train loss: 1.505 Accuracy:0.3066\n",
      "Epoch: 0/20 Iterarion:400 Train loss: 1.519 Accuracy:0.3223\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 0/20 Iterarion:405 Train loss: 1.520 Accuracy:0.3125\n",
      "Epoch: 1/20 Iterarion:410 Train loss: 1.493 Accuracy:0.3320\n",
      "Epoch: 1/20 Iterarion:415 Train loss: 1.542 Accuracy:0.2969\n",
      "Epoch: 1/20 Iterarion:420 Train loss: 1.529 Accuracy:0.2852\n",
      "Epoch: 1/20 Iterarion:425 Train loss: 1.543 Accuracy:0.3008\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 1/20 Iterarion:430 Train loss: 1.532 Accuracy:0.2930\n",
      "Epoch: 1/20 Iterarion:435 Train loss: 1.496 Accuracy:0.3516\n",
      "Epoch: 1/20 Iterarion:440 Train loss: 1.509 Accuracy:0.3555\n",
      "Epoch: 1/20 Iterarion:445 Train loss: 1.513 Accuracy:0.3145\n",
      "Epoch: 1/20 Iterarion:450 Train loss: 1.507 Accuracy:0.2969\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 1/20 Iterarion:455 Train loss: 1.524 Accuracy:0.2793\n",
      "Epoch: 1/20 Iterarion:460 Train loss: 1.506 Accuracy:0.3066\n",
      "Epoch: 1/20 Iterarion:465 Train loss: 1.497 Accuracy:0.3574\n",
      "Epoch: 1/20 Iterarion:470 Train loss: 1.514 Accuracy:0.3105\n",
      "Epoch: 1/20 Iterarion:475 Train loss: 1.504 Accuracy:0.3066\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 1/20 Iterarion:480 Train loss: 1.519 Accuracy:0.3125\n",
      "Epoch: 1/20 Iterarion:485 Train loss: 1.536 Accuracy:0.3145\n",
      "Epoch: 1/20 Iterarion:490 Train loss: 1.498 Accuracy:0.3438\n",
      "Epoch: 1/20 Iterarion:495 Train loss: 1.545 Accuracy:0.2930\n",
      "Epoch: 1/20 Iterarion:500 Train loss: 1.493 Accuracy:0.3945\n",
      "Val loss: 0.323 Val acc: 0.3105\n",
      "Epoch: 1/20 Iterarion:505 Train loss: 1.515 Accuracy:0.3320\n",
      "Epoch: 1/20 Iterarion:510 Train loss: 1.531 Accuracy:0.3184\n",
      "Epoch: 1/20 Iterarion:515 Train loss: 1.515 Accuracy:0.3008\n",
      "Epoch: 1/20 Iterarion:520 Train loss: 1.522 Accuracy:0.2988\n",
      "Epoch: 1/20 Iterarion:525 Train loss: 1.512 Accuracy:0.3320\n",
      "Val loss: 0.323 Val acc: 0.3105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-eaead8391047>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m                    \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                    initial_state:state}\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with sentiment_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session(graph=sentiment_graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state =  sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_sequence_batches(train_x, train_y, batch_size),1):\n",
    "            feed = {inputs_:x,\n",
    "                   labels_:y,\n",
    "                   keep_prob:0.8,\n",
    "                   initial_state:state}\n",
    "            loss, state, _, acc = sess.run([cost,final_state,optimizer,accuracy],feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                     \"Iterarion:{}\".format(iteration),\n",
    "                     \"Train loss: {:.3f}\".format(loss),\n",
    "                     \"Accuracy:{:.4f}\".format(acc))\n",
    "                \n",
    "            if iteration%25==0:\n",
    "                val_loss = []\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x,y in get_sequence_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_:x,\n",
    "                           labels_:y,\n",
    "                           keep_prob:1.0,\n",
    "                           initial_state:val_state}\n",
    "                    batch_loss,val_state,batch_acc = sess.run([cost,final_state,accuracy],feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                    val_loss.append(batch_loss)\n",
    "                    \n",
    "                print(\"Val loss: {:.3f}\".format(np.mean(val_loss)),\n",
    "                     \"Val acc: {:.4f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = []\n",
    "test_acc= []\n",
    "with tf.Session(graph=sentiment_graph) as sess:\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size,tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_sequence_batches(test_x, test_y, batch_size),1):\n",
    "        feed = {inputs_:x,\n",
    "               labels_:y,\n",
    "               keep_prob:1,\n",
    "               initial_state:test_state}\n",
    "        \n",
    "        batch_loss,test_state,batch_acc = sess.run([cost,final_state,accuracy],feed_dict=feed)\n",
    "        \n",
    "        test_loss.append(batch_loss)\n",
    "        test_acc.append(batch_acc)\n",
    "        \n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_loss)),\n",
    "         \"Test accuracy {:.4f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XIII 模型的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q31: 修改vocabulary size, embedding size, 并且结合使用LSTM， GRU， Bi-RNN， Stacked， Attentional, regularization, 等各种方法组合进行模型的优化， 至少进行10次优化，每次优化请按照以下步骤填写："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "---这是一个实例----\n",
    "\n",
    "第1次优化：\n",
    "\n",
    "1. 存在的问题： loss下降太慢；\n",
    "2. 准备进行的优化：减小模型的神经单元数量；\n",
    "3. 期待的结果：loss下降加快；\n",
    "4. 实际结果：loss下降的确加快(或者并没有加快)\n",
    "5. 原因分析：模型神经元数量减小，收敛需要的次数减少，loss下降加快\n",
    "\n",
    "\n",
    "---你的实验优化结构记录在此---\n",
    "\n",
    "**第1次优化**：\n",
    "\n",
    "1. 存在的问题： loss几乎不下降\n",
    "2. 准备进行的优化：learning rate batch size 增大，LSTM换为bidirectional-lstm\n",
    "3. 期待的结果：loss下降加快\n",
    "4. 实际结果：loss下降的确加快\n",
    "5. 原因分析：模型复杂度不够 欠拟合\n",
    "\n",
    "**第2次优化**：\n",
    "\n",
    "1. 存在的问题： loss开始下降，但是acc到0.5开始出现瓶颈\n",
    "2. 准备进行的优化：尝试加入attention机制，减少lstm size\n",
    "3. 期待的结果：loss下降加快，模型效果提升突破瓶颈\n",
    "4. 实际结果：loss下降并没有加快\n",
    "5. 原因分析：翻阅资料检查代码后，理解错公式发现实现的attention机制与论文有出入，计算score时的参数矩阵$W$,shape应该是(hidden_size,hidden_size)而不是(time_step,time_step)\n",
    "\n",
    "**第3次优化**：\n",
    "\n",
    "1. 存在的问题： 自己用keras实现的attention机制有问题（如第2次优化描述）\n",
    "2. 准备进行的优化：更改代码实现正确的attention机制\n",
    "3. 期待的结果：loss下降加快，模型效果提升突破瓶颈\n",
    "4. 实际结果：loss下降加快，模型效果提升突破瓶颈\n",
    "5. 原因分析：attention机制使模型更加专注于有用的信息进行预测，所以模型的效果更好\n",
    "\n",
    "**第4次优化**：\n",
    "\n",
    "1. 存在的问题： 模型在10 epochs后过拟合，train loss一直下降，但是valid loss不下降\n",
    "2. 准备进行的优化：在lstm加上0.2的drop out，keras模型拟合时加入early stopping机制，当val loss不下降时停止训练\n",
    "3. 期待的结果：train loss 和 valid loss 同步下降差距缩小\n",
    "4. 实际结果：确实缓解了过拟合的情况，但是loss下降得太慢，训练被early stopping机制停止\n",
    "5. 原因分析：单层的biLSTM模型还是不够复杂 模型还是欠拟合\n",
    "\n",
    "**第5次优化**：\n",
    "\n",
    "1. 存在的问题： 模型欠拟合，在接近10 epochs时loss不下降\n",
    "2. 准备进行的优化：增加模型深度，改为stacked 2 layer biLSTM\n",
    "3. 期待的结果：loss突破瓶颈继续下降而且避免过拟合\n",
    "4. 实际结果：loss并没有突破，还是有过拟合的情况\n",
    "5. 原因分析：模型过拟合\n",
    "\n",
    "**第6次优化**：\n",
    "\n",
    "1. 存在的问题： 模型过拟合，loss下降慢\n",
    "2. 准备进行的优化：增大drop out rate，减少lstm size,使用batchnorm\n",
    "3. 期待的结果：train loss 和 valid loss 同步下降差距缩小，acc能提高于0.5\n",
    "4. 实际结果：模型过拟合非常严重，loss也没有比之前下降很多\n",
    "5. 原因分析：加入batchnorm后模型太复杂\n",
    "\n",
    "**第7次优化**：\n",
    "\n",
    "1. 存在的问题： 模型过拟合 acc无法突破0.5\n",
    "2. 准备进行的优化：把原先的词向量参数trainable设置为True，也参与训练，同时调整batch size为32\n",
    "3. 期待的结果：解决过拟合问题 acc突破0.5\n",
    "4. 实际结果：仍然有严重的过拟合问题，但是acc突破了0.5\n",
    "5. 原因分析：模型已经足够复杂，要减少参数和运用正则化技巧\n",
    "\n",
    "**第9次优化**：\n",
    "\n",
    "1. 存在的问题： 模型过拟合严重\n",
    "2. 准备进行的优化：在bilstm层加上l2正则化\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第10次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XIV问题： 本次实验的总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请写实验的总结报告，描述此次项目的主要过程，其中遇到的问题，以及如何解决这些问题的，以及有什么经验和收获。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
