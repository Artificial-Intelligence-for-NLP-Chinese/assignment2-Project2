{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验指导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验目的：\n",
    "\n",
    "1. 掌握神经网络的基本原理；\n",
    "2. 掌握word2vec的基本原理；\n",
    "3. 掌握Keras的基本用法；\n",
    "4. 掌握tensorflow的基本用法；\n",
    "5. 掌握RNN的基本原理；\n",
    "6. 观察熟悉RNN的两种常见变体(LSTM和GRU)的原理和用法；\n",
    "7. 熟悉深度学习自然语言处理的基本流程。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I 实验描述：\n",
    "\n",
    "#### 数据： 使用爬虫获得的豆瓣评论数据\n",
    "#### 目标： 建立机器学习模型，能够对输入的句子自动判断其对应的分值或感情倾向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II 数据预处理\n",
    "\n",
    "目的： 将文本信息变成神经网络可以处理的数据格式； "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分： 基础理论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III 神经网络的基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: 神经网络的Loss函数的作用为何？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答： \n",
    "\n",
    "衡量神经网络预测值和真实值之间的误差，作为优化的目标函数，计算出来的误差值是反向传播的依据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: 神经网络的激活函数(activation function)起什么作用？ 如果没有激活函数会怎么样？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "1. 激活函数对输入数据进行了非线性变换。\n",
    "\n",
    "2. 若没有激活函数会导致模型过于简单，无法拟合非线性数据的复杂情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: 神经网络的softmax如何理解， 其作用是什么？ 在`答案`中写出softmax的python表达；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答案：\n",
    "\n",
    "softmax函数把神经网络输出的logits转化为总和为1的概率分布。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "logits = np.array([y1,y2,y3])\n",
    "softmax = np.exp(logits)/np.sum(np.exp(logits))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: 简述 normalized_1 和softmax函数的相同点和不同点， 说明softmax相比normalized_1该函数的优势所在\n",
    "```python\n",
    "output = np.array([y1, y2, y3])\n",
    "\n",
    "normalized_1 = output / np.sum(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "相同点：normalized_1和softmax函数都是数据归一化的方式。\n",
    "\n",
    "不同点：softmax在标准化之前对数据进行了指数转换。\n",
    "\n",
    "softmax的优势：可以处理output中同时存在正负数的情况，数据间评分的概率分布不会在计算时抵消。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5: 写出crossentropy的函数表达式，说明该函数的作用和意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "cross_entropy $:= -\\sum_{c=1}^{C}y_{c}\\log{\\hat{y}_{c}}$\n",
    "\n",
    "在分类问题中对比预测值和真实值衡量它们之间的误差。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV 掌握word2vec的基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6: 说明word2vec要解决的问题背景， 以及word2vec的基本思路， 说明word2vec比起之前方法的优势；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "问题背景：\n",
    "1. 对于简单的方法，词向量化质量低，没有很好地保持不同词语之间的语义关系。\n",
    "2. 过于复杂的方法，当需要向量化的词汇太多时计算量过大，效率低。\n",
    "\n",
    "基本思路：\n",
    "1. 对比不同模型**NNLM**,**RNNLM**的计算复杂度，非线性隐藏层构成了主要的计算复杂度，需要提出一个更有效率的模型。\n",
    "2. Log-linear models:**CBOW**和**Skip-gram**,两个模型的网络结构都相同，都是一层隐藏层作线性投影后再用log-linear输出分类结果,不同的是**CBOW**是用前后内容预测当前词，而**Skip-gram**是用当前词预测围绕再其附近的词。\n",
    "\n",
    "优势：\n",
    "1. 减少了计算复杂度的同时，提高了模型准确率。\n",
    "2. 更进一步地，训练出来的词向量能以简单代数运算的方式表达词义，例如Paris - France + Italy = Rome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7: 说明word2vec的预测目标， predication target, 在答案中写出skip-gram和cbow的预测概率；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答： \n",
    "1.  skip-gram, $\\hat{y}= \\{P(w_{t-i}|w_t),P(w_{t-i+1}|w_t),\\cdots P(w_{t-1}|w_t),P(w_{t+1}|w_t) \\cdots ,P(w_{t+i}|w_t)\\}$\n",
    "2.  cbow, $\\hat{y} = P(w_t|w_{t-i},w_{t-i+1},\\cdots w_{t-1},w_{t-1}\\cdots,w_{t+i})$\n",
    "\n",
    "$i$ is window size\n",
    "\n",
    "hints: 你可能需要查询latex的基本写法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8: 请说明word2vec的两种常见优化方法，分别阐述其原理；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "\n",
    "1. Hierachical softmax. 将原本要预测的词汇集合$V$转化为binary tree的搜索路径，每一个词都对应着一条唯一的路径，预测词出现的概率就相当于预测这条路径出现的概率。这样作当更新参数时，复杂度从原来的$O(V)$减少到了$O(\\log{V})$，大大较少了计算复杂度。\n",
    "\n",
    "2.Negative sampling. 由于需要更新的参数太多，在进行优化时仅仅更新正类和抽样地更新某些负类有关的参数，其中更新的负类可以是随机抽样的，也可以是根据词频调整的函数分布抽样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9: 请说明word2vec中哈夫曼树的作用；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "word2vec使用hierachical softmax进行优化时，需要把预测的词汇转化为binary tree，这个时候通常会采用huffman binary tree。在构造huffman binary tree时，出现频率越高的词汇约接近根节点，而出现频率越低的词汇越接近叶节点，并且保证每个父节点都有两个子节点。在计算结果和优化word2vec时需要遍历从根节点到每个子节点的唯一路径，而只有很少的情况下会达到深层的叶子节点，所以huffman binary tree提高了模型的运算效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10: 在gensim中如何实现词向量？ 请将gensim中实现词向量的代码置于`答案`中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentence =['some sentences']\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11: 请说出除了 skip-gram和cbow的其他4中词向量方法的名字， 并且选取其中两个叙述其基本原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "ont-hot,count vector,co-occurrence vector,tf-idf\n",
    "\n",
    "co-occurence vector:\n",
    "在由两个词在窗口范围内共同出现次数构成的矩阵，通过svd变换后求得的词向量。\n",
    "\n",
    "tf-idf: term frequency-inverse document frequency\n",
    "\n",
    "$TF_i = \\frac{c_{i}}{C_d} $ 词i出现在文档中的频率\n",
    "\n",
    "$IDF_i = \\log({\\frac{N}{c_{i \\in d}}} )$ 总文档/词i出现的文档数\n",
    "\n",
    "$TF-IDF_i = TF_i * IDF_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V 掌握keras的基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12. 参考keras参考手册，构建一个机器学习模型，该模型能够完成使用DNN(deep neural networks) 实现MNIST数据集的分类；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：代码请置于下方：\n",
    "\n",
    "hints:  keras 序列模型构建 https://keras.io/getting-started/sequential-model-guide/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=np.load('/Users/mozhiwen/.keras/datasets/mnist/mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test = f['x_train'],f['y_train'],f['x_test'],f['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(dataset,labels):\n",
    "    dataset = dataset.reshape(-1,img_size*img_size).astype('float32')\n",
    "    dataset = (dataset-np.min(dataset))/(np.max(dataset)-np.min(dataset))\n",
    "    labels = keras.utils.to_categorical(labels[:,None], num_classes)\n",
    "    return dataset,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train = reformat(x_train,y_train)\n",
    "x_test,y_test = reformat(x_test,y_test)\n",
    "print(x_train.shape[0],'train samples')\n",
    "print(x_test.shape[0],'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512,activation='relu',input_shape=(img_size*img_size,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.2471 - acc: 0.9237 - val_loss: 0.1005 - val_acc: 0.9687\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1046 - acc: 0.9691 - val_loss: 0.0744 - val_acc: 0.9763\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0769 - acc: 0.9770 - val_loss: 0.0753 - val_acc: 0.9768\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.0607 - acc: 0.9817 - val_loss: 0.0773 - val_acc: 0.9802\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0512 - acc: 0.9846 - val_loss: 0.0796 - val_acc: 0.9789\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0433 - acc: 0.9871 - val_loss: 0.0694 - val_acc: 0.9827\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0398 - acc: 0.9889 - val_loss: 0.0777 - val_acc: 0.9825\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0323 - acc: 0.9903 - val_loss: 0.0815 - val_acc: 0.9809\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0323 - acc: 0.9910 - val_loss: 0.0963 - val_acc: 0.9808\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0294 - acc: 0.9910 - val_loss: 0.0860 - val_acc: 0.9819\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0275 - acc: 0.9923 - val_loss: 0.0897 - val_acc: 0.9823\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0250 - acc: 0.9932 - val_loss: 0.1038 - val_acc: 0.9816\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.0246 - acc: 0.9931 - val_loss: 0.1057 - val_acc: 0.9825\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0215 - acc: 0.9943 - val_loss: 0.1016 - val_acc: 0.9821\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.0241 - acc: 0.9935 - val_loss: 0.1068 - val_acc: 0.9810\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0218 - acc: 0.9941 - val_loss: 0.0926 - val_acc: 0.9847\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0196 - acc: 0.9952 - val_loss: 0.1092 - val_acc: 0.9816\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0221 - acc: 0.9940 - val_loss: 0.1044 - val_acc: 0.9836\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0205 - acc: 0.9946 - val_loss: 0.1141 - val_acc: 0.9848\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0213 - acc: 0.9946 - val_loss: 0.0974 - val_acc: 0.9842\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                    verbose = 1,\n",
    "                    validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0973729898917102\n",
      "Test accuracy: 0.9842\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:',score[0])\n",
    "print('Test accuracy:',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI 掌握tensorflow的基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13: 参考tensorflow的参考手册，构建一个机器学习模型，该模型能够完成使用DNN(deep neural networks)实现MNIST数据集的分类；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：代码请置于下方：\n",
    "\n",
    "hints:tensorflow实现MNIST https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #input\n",
    "    train_data = tf.placeholder(tf.float32,shape=(None,img_size*img_size))\n",
    "    train_labels = tf.placeholder(tf.float32,shape=(None,num_classes))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    test_data = tf.constant(x_test)\n",
    "    test_labels = tf.constant(y_test)\n",
    "    \n",
    "    #variables\n",
    "    w1 = tf.Variable(tf.truncated_normal([img_size * img_size, 512]))\n",
    "    b1 = tf.Variable(tf.zeros([512]))\n",
    "    w2 = tf.Variable(tf.truncated_normal([512, 512]))\n",
    "    b2 = tf.Variable(tf.zeros([512]))\n",
    "    w3 = tf.Variable(tf.truncated_normal([512, num_classes]))\n",
    "    b3 = tf.Variable(tf.zeros([num_classes]))\n",
    "    \n",
    "    #model\n",
    "    layer1 = tf.nn.relu(train_data@w1+b1)\n",
    "    drop_layer1 = tf.nn.dropout(layer1,keep_prob=keep_prob)\n",
    "    layer2 = tf.nn.relu(drop_layer1@w2+b2)\n",
    "    drop_layer2 = tf.nn.dropout(layer2,keep_prob=keep_prob)\n",
    "    logits = drop_layer2@w3+b3\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=train_labels,logits=logits))\n",
    "    \n",
    "    optimizer = tf.train.RMSPropOptimizer(0.001,decay=0.0).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    test_out1 = tf.nn.relu(test_data@w1+b1)\n",
    "    test_out2 = tf.nn.relu(test_out1@w2+b2)\n",
    "    test_logits = test_out2@w3+b3\n",
    "    test_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=test_labels,logits=test_logits))\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(dataset,labels,batch_size):\n",
    "    batch_num = dataset.shape[0] // batch_size\n",
    "    sample_num = batch_size * batch_num\n",
    "    for i in range(batch_num):\n",
    "        x = dataset[i*batch_size:(i+1)*batch_size,:]\n",
    "        y = labels[i*batch_size:(i+1)*batch_size,:]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 58.746006 accuracy 90.6%\n",
      "epoch 1 loss: 21.229315 accuracy 93.0%\n",
      "epoch 2 loss: 25.321468 accuracy 92.2%\n",
      "epoch 3 loss: 22.862274 accuracy 93.8%\n",
      "epoch 4 loss: 11.274590 accuracy 94.5%\n",
      "epoch 5 loss: 7.947142 accuracy 96.9%\n",
      "epoch 6 loss: 2.745044 accuracy 97.7%\n",
      "epoch 7 loss: 10.025577 accuracy 96.9%\n",
      "epoch 8 loss: 1.138198 accuracy 97.7%\n",
      "epoch 9 loss: 7.077971 accuracy 97.7%\n",
      "epoch 10 loss: 4.300566 accuracy 96.9%\n",
      "epoch 11 loss: 0.718770 accuracy 98.4%\n",
      "epoch 12 loss: 3.604064 accuracy 97.7%\n",
      "epoch 13 loss: 6.006754 accuracy 96.9%\n",
      "epoch 14 loss: 3.026232 accuracy 96.9%\n",
      "epoch 15 loss: 2.748903 accuracy 98.4%\n",
      "epoch 16 loss: 2.132647 accuracy 98.4%\n",
      "epoch 17 loss: 2.855407 accuracy 96.1%\n",
      "epoch 18 loss: 4.587665 accuracy 96.9%\n",
      "epoch 19 loss: 1.937775 accuracy 96.9%\n",
      "Test loss: 5.906247 accuracy 96.0%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    step = 0\n",
    "    for i in range(epochs):\n",
    "        for x,y in get_batches(x_train,y_train,batch_size):\n",
    "            feed_dict = {train_data:x,train_labels:y,keep_prob:0.8}\n",
    "            _,l,predictions = session.run([optimizer,loss,train_prediction],\n",
    "                                                feed_dict=feed_dict)\n",
    "        print('epoch %d loss: %f accuracy %.1f%%' % (i,l,accuracy(predictions, y)))\n",
    "        \n",
    "    print('Test loss: %f accuracy %.1f%%' % (test_loss.eval(),accuracy(test_prediction.eval(), y_test)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q14: 参考keras和tensorflow对同一问题的实现，说明keras和tensorflow的异同；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：Q12,Q12分别用keras和tensorflow实现了dnn识别mnist数据集。相同点是都需要自己定义模型的输入输出，损失函数和优化器。不同点是相比起tensorflow，keras提供了许多常用的api，实现起来代码量少，代码实现的模型简洁易懂，比较简便省事，不需要对模型和编程有深入的了解也能使用。而tensrflow相比起来更加强大，提供了很多在keras下无法掌控的个性化的参数设置，实现同一个任务代码量比较多，使用要求对模型和框架理解比较深入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q15: Q12， Q13 的tensorflow 或 keras 模型的训练时准确率和测试集准确率分别是多少？\n",
    "回答：\n",
    "tensorflow：训练准确率：96.9% 测试准确率：96.0%\n",
    "keras：训练准确率：97.4% 测试准确率：98.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q16: 训练时准确率大于测试集准确率的现象叫什么名字，在神经网络中如何解决该问题？\n",
    "回答：过拟合。使用early stopping,l2-regularization,drop out。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q17: 请使用自己的语言简述通过正则化 (regularization)减小过拟合的原理；\n",
    "回答：\n",
    "\n",
    "1. early stopping是在训练时观察training set 和 validation set的表现当validation set的表现无明显提升时停止训练。\n",
    "\n",
    "2. l2-regularizarion,改写loss function为 $loss = f_{loss}+\\lambda\\lVert \\omega \\rVert_{2}^{2} $ 其中$f_{loss}$是未加上正则项之前的loss function，$\\lambda\\lVert \\omega \\rVert_{2}^{2}$是正则项，$\\omega$是模型参数。加上了正则项的训练函数，在训练会权衡误差的下降和参数增加对loss fuction的影响，在下降误差的同时尽量使模型参数变得稀疏，避免过拟合。\n",
    "\n",
    "3. 训练阶段，在深度学习模型层与层的输出和输入之间按比例随机切断一些链接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q18: 在tensorflow官方实例中给出的fully connected 神经网络的分类模型中，数据进行了哪些预处理，这些预处理的原因是什么？ \n",
    "回答：对数据进行了标准化，标准化后的数据不受features单位影响，训练时收敛得比较快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII 掌握RNN的基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q19: 简述RNN解决的问题所具有的特点；\n",
    "回答：RNN解决问题的特点是，处理的输入或输出数据都是时间或者序列相关，不同时间位置的数据可能相互关联相互影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q20: 写出RNN实现时间或者序列相关的数学实现(见课程slides)；\n",
    "回答：\n",
    "\n",
    "Elman network:\n",
    "\n",
    "$h_t = \\sigma_{h}({W_hx_t+U_hh_{t-1}+b_h})$\n",
    "\n",
    "$y_t = \\sigma_y(W_yh_t+b_y)$\n",
    "\n",
    "Jordan network:\n",
    "\n",
    "$h_t = \\sigma_h(W_hx_t+U_hy_{t-1}+b_h)$\n",
    "\n",
    "$y_t = \\sigma_y(W_yh_t+b_y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q20: 简述RNN的两种重要变体的提出原因和基本原理？\n",
    "回答：\n",
    "\n",
    "提出原因：\n",
    "\n",
    "在输入长序列训练RNN时，需要展开计算多次使得RNN变成一个很深层的网络，所以非常容易出现梯度消失或梯度爆炸的问题，而且模型的计算和训练会非常缓慢。另一方面，RNN处理长序列时会丧失长期记忆的能力，序列中较前的输入会逐渐被遗忘，这对模型的效果会造成很大的影响。\n",
    "\n",
    "基本原理:\n",
    "\n",
    "LSTM:\n",
    "\n",
    "- 最主要的一层是$g_t$,这层和基础RNN cell基本相同，都是用过$x_t$和$h_{t-1}$得出结果。\n",
    "在此基础上还有三种gate contrillers，用sigmoid函数输出0-1的结果来控制输入输出。\n",
    "- forget gate 控制长期记忆中的哪些部分需要舍弃\n",
    "- input gate 控制 $g_t$中的哪些部分需要加到长期记忆中\n",
    "- output gate 控制更新后的长期记忆哪一部分输出到$h_t$,$y_t$中\n",
    "\n",
    "GRU:\n",
    "\n",
    "- GRU是LSTM的精简版，它把长短期记忆都融合成一个状态$h_t$\n",
    "- 由一个controller同时控制forget gate和input gate\n",
    "- 没有output gate 状态会根据序列时间改变\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q21: Attentional RNN 以及 Stacked RNN 和 Bi-RNN 分别是什么，其做了什么改动？\n",
    "回答：\n",
    "\n",
    "Attentional RNN:  \n",
    "\n",
    "在计算输出值时的在考虑原有输入的同时，加入了context vector $s_{i-1} = f(s_{i-1},y_{i-1},c_i)$,其中$c_i$是源输入的$h_t$的加权平均$c_i = \\sum_{j=1}^{T_x}\\alpha_{ij}h_j$,而$\\alpha_{ij} = softmax(e_{ij})$, $e_{ij} = a(s_{i-1},h_j)$,其中a是一个全链接神经网络。这样做可以建立在计算输出时，建立与源输入的联系，并且专注与这一时刻有用的信息。  \n",
    "\n",
    "Stacked RNN:\n",
    "\n",
    "可以理解为RNN cell的堆叠，t时刻的的输出不仅仅是t+1时刻的输入，同时候也是下一层RNN cell的输入，这样可以根据需要增加网络深度。  \n",
    "\n",
    "Bi-RNN:\n",
    "\n",
    "在原有RNN的基础上，计算从后往前的RNN cell，t时刻的预测值就包含了输入序列的前后信息。t时刻的预测可表示为$\\hat{y_t} = \\sigma_y(w_y[\\overrightarrow{h_t},\\overleftarrow{h_t}]+b_y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分： 实验过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIX 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q22：要实现文本分类或情感分类，文本信息需要进行哪些初始化操作？自己手工实现，keras提供的API，tenorflow提供的API，分别是哪些？请提供关键代码置于下边`回答`中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "1. 简述所需要的初始化操作：\n",
    "2. 自己手工实现的id_to_word, word_to_id, padding, batched等操作如何实现？\n",
    "    + id_to_word, word_to_id\n",
    "    \n",
    "    ```python\n",
    "    python: <you code here>\n",
    "    ```\n",
    "    + padding\n",
    "    \n",
    "    ```python\n",
    "    python: <you code here>\n",
    "    ```\n",
    "    \n",
    "    + batched\n",
    "    \n",
    "    ```python\n",
    "    python: <you code here>\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hints: \n",
    "\n",
    "+ 参考1 https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "+ 参考2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX 构建神经网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q22:在没有预训练的词向量时候， keras 如何实现embedding操作，即如何依据一个单词的序列获得其向量表示？\n",
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Q23:在没有预训练的词向量时候， tensorflow 如何实现embedding操作，即如何依据一个单词的序列获得其向量表示？\n",
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q24: 在有预先训练的词向量时候，keras和tensorflow又如何实现embeding操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q25： 基于上文进行的数据预处理，使用keras和tensorflow如何构建神经网络模型？请提供关键代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答： keras模型构建的关键代码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：tensorflow模型构建的关键代码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  X 使用keras的history观察loss以及accuracy的变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q26: keras如何观察模型的loss变化以及准确率的变化，请列出关键代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q27: 请使用matplotlib画出loss变化的趋势；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XI 使用tensorflow tensorboard观察loss, accuracy的变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q28: tensorflow如何观察模型的loss变化以及准确率的变化， tensor board 如何使用？ 请列出关键代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q29: 试着点击tensor board的不同按钮 观察图像的变化； 试着给tensorflow board机制 写入不同时候训练的模型时候，给模型取不同的名字，观察tensor board的图像变化；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XII 观察熟悉RNN的两种变体的原理和方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q30:试着改进RNN，使用LSTM， GRU 进行模型的改动， 观察训练结果(loss和accuracy)的变化， 你观察到了什么变化？ 如何解释？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XIII 模型的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q31: 修改vocabulary size, embedding size, 并且结合使用LSTM， GRU， Bi-RNN， Stacked， Attentional, regularization, 等各种方法组合进行模型的优化， 至少进行10次优化，每次优化请按照以下步骤填写："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "---这是一个实例----\n",
    "\n",
    "第1次优化：\n",
    "\n",
    "1. 存在的问题： loss下降太慢；\n",
    "2. 准备进行的优化：减小模型的神经单元数量；\n",
    "3. 期待的结果：loss下降加快；\n",
    "4. 实际结果：loss下降的确加快(或者并没有加快)\n",
    "5. 原因分析：模型神经元数量减小，收敛需要的次数减少，loss下降加快\n",
    "\n",
    "\n",
    "---你的实验优化结构记录在此---\n",
    "\n",
    "**第1次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第2次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第3次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第4次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第5次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第6次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第7次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第9次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第10次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XIV问题： 本次实验的总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请写实验的总结报告，描述此次项目的主要过程，其中遇到的问题，以及如何解决这些问题的，以及有什么经验和收获。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
